{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "train_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\dataset.csv\"\n",
        "test_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\test.csv\"\n",
        "sam_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\sample_submission.csv\"\n",
        "\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000, 47)\n",
            "Index(['id', 'sale_date', 'sale_price', 'sale_nbr', 'sale_warning',\n",
            "       'join_status', 'join_year', 'latitude', 'longitude', 'area', 'city',\n",
            "       'zoning', 'subdivision', 'present_use', 'land_val', 'imp_val',\n",
            "       'year_built', 'year_reno', 'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
            "       'grade', 'fbsmt_grade', 'condition', 'stories', 'beds', 'bath_full',\n",
            "       'bath_3qtr', 'bath_half', 'garb_sqft', 'gara_sqft', 'wfnt', 'golf',\n",
            "       'greenbelt', 'noise_traffic', 'view_rainier', 'view_olympics',\n",
            "       'view_cascades', 'view_territorial', 'view_skyline', 'view_sound',\n",
            "       'view_lakewash', 'view_lakesamm', 'view_otherwater', 'view_other',\n",
            "       'submarket'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sale_date</th>\n",
              "      <th>sale_price</th>\n",
              "      <th>sale_nbr</th>\n",
              "      <th>sale_warning</th>\n",
              "      <th>join_status</th>\n",
              "      <th>join_year</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>area</th>\n",
              "      <th>...</th>\n",
              "      <th>view_olympics</th>\n",
              "      <th>view_cascades</th>\n",
              "      <th>view_territorial</th>\n",
              "      <th>view_skyline</th>\n",
              "      <th>view_sound</th>\n",
              "      <th>view_lakewash</th>\n",
              "      <th>view_lakesamm</th>\n",
              "      <th>view_otherwater</th>\n",
              "      <th>view_other</th>\n",
              "      <th>submarket</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-15</td>\n",
              "      <td>236000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.2917</td>\n",
              "      <td>-122.3658</td>\n",
              "      <td>53</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1999-01-15</td>\n",
              "      <td>313300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.6531</td>\n",
              "      <td>-122.1996</td>\n",
              "      <td>74</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2006-08-15</td>\n",
              "      <td>341000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4733</td>\n",
              "      <td>-122.1901</td>\n",
              "      <td>30</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1999-12-15</td>\n",
              "      <td>267000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4739</td>\n",
              "      <td>-122.3295</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-15</td>\n",
              "      <td>1650000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>miss99</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.7516</td>\n",
              "      <td>-122.1222</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 47 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id   sale_date  sale_price  sale_nbr sale_warning join_status  join_year  \\\n",
              "0   0  2014-11-15      236000       2.0                    nochg       2025   \n",
              "1   1  1999-01-15      313300       NaN          26        nochg       2025   \n",
              "2   2  2006-08-15      341000       1.0                    nochg       2025   \n",
              "3   3  1999-12-15      267000       1.0                    nochg       2025   \n",
              "4   4  2018-07-15     1650000       2.0                   miss99       2025   \n",
              "\n",
              "   latitude  longitude  area  ... view_olympics view_cascades  \\\n",
              "0   47.2917  -122.3658    53  ...             0             0   \n",
              "1   47.6531  -122.1996    74  ...             0             0   \n",
              "2   47.4733  -122.1901    30  ...             0             0   \n",
              "3   47.4739  -122.3295    96  ...             0             0   \n",
              "4   47.7516  -122.1222    36  ...             0             0   \n",
              "\n",
              "  view_territorial  view_skyline  view_sound  view_lakewash  view_lakesamm  \\\n",
              "0                0             0           0              0              0   \n",
              "1                0             0           0              1              0   \n",
              "2                0             0           0              0              0   \n",
              "3                0             0           0              0              0   \n",
              "4                0             0           0              0              0   \n",
              "\n",
              "   view_otherwater  view_other  submarket  \n",
              "0                0           0          I  \n",
              "1                0           0          Q  \n",
              "2                0           0          K  \n",
              "3                0           0          G  \n",
              "4                0           0          P  \n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "sample_df = pd.read_csv(sam_path)\n",
        "\n",
        "print(train_df.shape)\n",
        "print(train_df.columns)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sale_nbr       42182\n",
              "subdivision    17550\n",
              "submarket       1717\n",
              "dtype: int64"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#è¨ˆç®—ç¼ºå¤±å€¼\n",
        "missing_values = train_df.isnull().sum()\n",
        "missing_values[missing_values > 0].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å»ºç«‹ train_encoded\n",
        "processed_cols = []\n",
        "train_encoded = pd.DataFrame()\n",
        "\n",
        "# One-hot \n",
        "confirmed_one_hot = [\n",
        "    'join_status', 'condition', 'stories', 'grade', 'fbsmt_grade', 'present_use'\n",
        "]\n",
        "onehot_df = pd.get_dummies(train_df[confirmed_one_hot], drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, onehot_df], axis=1)\n",
        "processed_cols += confirmed_one_hot\n",
        "\n",
        "\n",
        "train_df['sale_date'] = pd.to_datetime(train_df['sale_date'], errors='coerce')\n",
        "train_encoded['sale_year'] = train_df['sale_date'].dt.year\n",
        "train_encoded['sale_month'] = train_df['sale_date'].dt.month\n",
        "train_encoded['sale_season'] = ((train_encoded['sale_month'] - 1) // 3 + 1)\n",
        "processed_cols += ['sale_date']\n",
        "\n",
        "# åŽŸå§‹æ•¸å€¼ç›´æŽ¥åŠ å…¥\n",
        "direct_add_cols = [\n",
        "    'id', 'sale_price', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half', 'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'  to much missing value\n",
        "]\n",
        "for col in direct_add_cols:\n",
        "    train_encoded[col] = train_df[col]\n",
        "processed_cols += direct_add_cols\n",
        "\n",
        "# çµ±æ•´åŸŽå¸‚ã€å¸‚å ´èˆ‡éŠ·å”®è­¦å‘Šè³‡è¨Š\n",
        "top_cities = train_df['city'].value_counts().nlargest(10).index.tolist()\n",
        "top_supermarket = train_df['submarket'].value_counts().nlargest(10).index.tolist()\n",
        "top_sale_warning = train_df['sale_warning'].value_counts().nlargest(15).index.tolist()\n",
        "\n",
        "train_encoded['city_simplified'] = train_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "train_encoded['submarket_simplified'] = train_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "train_encoded['sale_warning_simplified'] = train_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "\n",
        "city_dummy = pd.get_dummies(train_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "submarket_dummy = pd.get_dummies(train_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "sale_warning_dummy = pd.get_dummies(train_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, city_dummy, submarket_dummy, sale_warning_dummy], axis=1)\n",
        "processed_cols += ['city', 'submarket', 'sale_warning']\n",
        "\n",
        "# Zoning ç¾¤çµ„åˆ†é¡ž\n",
        "def zoning_group_classify(z):\n",
        "    if pd.isna(z): return 'other'\n",
        "    z = z.upper()\n",
        "    if 'SF' in z: return 'SF'\n",
        "    elif 'MR' in z: return 'MR'\n",
        "    elif 'NC' in z: return 'NC'\n",
        "    elif 'HR' in z or 'IG' in z: return 'other'\n",
        "    elif 'P' in z: return 'P'\n",
        "    return 'other'\n",
        "\n",
        "train_encoded['zoning_group'] = train_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(train_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, zoning_dummy], axis=1)\n",
        "train_encoded.drop(columns=['zoning_group'], inplace=True)\n",
        "processed_cols += ['zoning']\n",
        "\n",
        "\n",
        "# ç¢Žç‰‡åŒ–è³‡è¨Šçµ±æ•´æˆæ–°æ¬„ä½\n",
        "train_encoded['age'] = train_encoded['sale_year'] - train_encoded['year_built']\n",
        "train_encoded['renovated'] = np.where(train_encoded['year_reno'] > 0, 1, 0)\n",
        "train_encoded['years_since_reno'] = np.where(train_encoded['renovated'], train_encoded['sale_year'] - train_encoded['year_reno'], 0)\n",
        "train_encoded['total_baths'] = train_encoded['bath_full'] + 0.75 * train_encoded['bath_3qtr'] + 0.5 * train_encoded['bath_half']\n",
        "train_encoded['total_value'] = train_encoded['land_val'] + train_encoded['imp_val']\n",
        "train_encoded['living_area'] = train_encoded['sqft'] + train_encoded['sqft_fbsmt']\n",
        "\n",
        "# åˆªé™¤ç”¨å®Œçš„ç°¡åŒ–æ–‡å­—é¡žæ¬„ä½\n",
        "for col in ['city_simplified', 'submarket_simplified', 'sale_warning_simplified']:\n",
        "    train_encoded.drop(columns=[col], inplace=True)\n",
        "\n",
        "\n",
        "#æ–°å¢žç‰¹å¾µ\n",
        "non_zero_lot = train_encoded.loc[train_encoded[\"sqft_lot\"] > 0, \"sqft_lot\"]\n",
        "min_val = non_zero_lot.min()\n",
        "median_val = non_zero_lot.median()\n",
        "\n",
        "train_encoded[\"sqft_lot\"] = train_encoded[\"sqft_lot\"].replace(0, median_val)\n",
        "\n",
        "#æ–°å¢žç‰¹å¾µ\n",
        "train_encoded[\"floor_ratio\"] = np.where(\n",
        "    train_encoded[\"sqft_lot\"] == 0,\n",
        "    0,\n",
        "    train_encoded[\"sqft\"] / train_encoded[\"sqft_lot\"]\n",
        ")\n",
        "\n",
        "train_encoded[\"is_large_house\"] = (train_encoded[\"sqft\"] > 3000).astype(int)\n",
        "train_encoded[\"is_recent_reno\"] = (train_encoded[\"years_since_reno\"] <= 5).astype(int)\n",
        "train_encoded[\"bath_per_bed\"] = train_encoded[\"total_baths\"] / train_encoded[\"beds\"]\n",
        "train_encoded[\"bath_per_bed\"] = train_encoded[\"bath_per_bed\"].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "# å±‹é½¡å€é–“\n",
        "train_encoded[\"age_bin\"] = pd.cut(\n",
        "    train_encoded[\"age\"],\n",
        "    bins=[-1, 10, 30, 60, 200],\n",
        "    labels=[\"0-10\", \"11-30\", \"31-60\", \"60+\"]\n",
        ")\n",
        "age_dummies = pd.get_dummies(train_encoded[\"age_bin\"], prefix=\"age_bin\")\n",
        "train_encoded = pd.concat([train_encoded, age_dummies], axis=1)\n",
        "train_encoded.drop(columns=[\"age_bin\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_features = ['latitude', 'longitude', 'area', 'sqft', 'total_value']\n",
        "\n",
        "# æ¨™æº–åŒ–\n",
        "scaler = StandardScaler()\n",
        "train_cluster_scaled = scaler.fit_transform(train_encoded[cluster_features])\n",
        "\n",
        "# å»ºç«‹ KMeans ç¾¤çµ„ï¼ˆå»ºè­°å…ˆå¾ž 10 ç¾¤é–‹å§‹ï¼‰\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init='auto')\n",
        "train_encoded['region_cluster'] = kmeans.fit_predict(train_cluster_scaled)\n",
        "\n",
        "# One-hot ç·¨ç¢¼\n",
        "cluster_ohe = pd.get_dummies(train_encoded['region_cluster'], prefix=\"region\")\n",
        "train_encoded = pd.concat([train_encoded, cluster_ohe], axis=1)\n",
        "\n",
        "# ç§»é™¤åŽŸå§‹ cluster idï¼ˆå› ç‚ºæ˜¯é¡žåˆ¥ï¼‰\n",
        "train_encoded.drop(columns=['region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "pca_features = ['latitude', 'longitude', 'sqft', 'area', 'total_value', 'imp_val']\n",
        "\n",
        "# ðŸ”ƒ æ¨™æº–åŒ– â†’ PCA â†’ KMeans\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(train_encoded[pca_features])\n",
        "\n",
        "pca = PCA(n_components=3, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "train_encoded['pca_region_cluster'] = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# ðŸ”„ One-hot ç·¨ç¢¼\n",
        "region_dummies = pd.get_dummies(train_encoded['pca_region_cluster'], prefix='pca_region')\n",
        "train_encoded = pd.concat([train_encoded, region_dummies], axis=1)\n",
        "\n",
        "train_encoded.drop(columns=['pca_region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_features(df):\n",
        "    import numpy as np\n",
        "\n",
        "    # å»ºè­° log1p è™•ç†ï¼ˆé¿å…æ¥µç«¯åæ…‹å½±éŸ¿æ¨¡åž‹ï¼‰\n",
        "    log_cols = ['land_val', 'imp_val', 'sqft_lot', 'garb_sqft', 'floor_ratio', 'total_value']\n",
        "    for col in log_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.log1p(df[col])\n",
        "\n",
        "    # clip ä¸Šé™å€¼ï¼ˆå¯é¸ï¼Œå¦‚æžœä½ ä¸ logï¼‰\n",
        "    clip_cols = ['land_val', 'imp_val', 'sqft_lot']\n",
        "    for col in clip_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].clip(upper=1_000_000)\n",
        "            \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_encoded = clean_features(train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "direct_add_cols = [\n",
        "    'id', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half',\n",
        "    'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'æ²’æœ‰åšé€™å€‹ ç”¨æ„ä¸å¤§\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å»ºç«‹ test_encoded ç©ºè¡¨\n",
        "test_encoded = pd.DataFrame()\n",
        "\n",
        "# 1. One-hot æ¬„ä½\n",
        "test_onehot_df = pd.get_dummies(test_df[confirmed_one_hot], drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, test_onehot_df], axis=1)\n",
        "\n",
        "# 2. æ—¥æœŸè™•ç†\n",
        "test_df['sale_date'] = pd.to_datetime(test_df['sale_date'], errors='coerce')\n",
        "test_encoded['sale_year'] = test_df['sale_date'].dt.year\n",
        "test_encoded['sale_month'] = test_df['sale_date'].dt.month\n",
        "test_encoded['sale_season'] = ((test_encoded['sale_month'] - 1) // 3 + 1)\n",
        "\n",
        "# 3. åŠ å…¥ direct_add_cols æ¬„ä½\n",
        "for col in direct_add_cols:\n",
        "    test_encoded[col] = test_df[col]\n",
        "\n",
        "# 4. city / submarket / sale_warning (simplified)\n",
        "test_encoded['city_simplified'] = test_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "city_dummy = pd.get_dummies(test_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, city_dummy], axis=1)\n",
        "\n",
        "test_encoded['submarket_simplified'] = test_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "submarket_dummy = pd.get_dummies(test_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, submarket_dummy], axis=1)\n",
        "\n",
        "test_encoded['sale_warning_simplified'] = test_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "sale_warning_dummy = pd.get_dummies(test_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, sale_warning_dummy], axis=1)\n",
        "\n",
        "# 5. Zoning åˆ†ç¾¤ One-hot\n",
        "test_encoded['zoning_group'] = test_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(test_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, zoning_dummy], axis=1)\n",
        "test_encoded.drop(columns=['zoning_group', 'city_simplified', 'submarket_simplified', 'sale_warning_simplified'], inplace=True)\n",
        "\n",
        "\n",
        "#ç¢Žç‰‡åŒ–è³‡è¨Šçµ±æ•´æˆæ–°æ¬„ä½\n",
        "test_encoded['age'] = test_encoded['sale_year'] - test_encoded['year_built']\n",
        "test_encoded['renovated'] = np.where(test_encoded['year_reno'] > 0, 1, 0)\n",
        "test_encoded['years_since_reno'] = np.where(test_encoded['renovated'], test_encoded['sale_year'] - test_encoded['year_reno'], 0)\n",
        "test_encoded['total_baths'] = test_encoded['bath_full'] + 0.75 * test_encoded['bath_3qtr'] + 0.5 * test_encoded['bath_half']\n",
        "test_encoded['total_value'] = test_encoded['land_val'] + test_encoded['imp_val']\n",
        "test_encoded['living_area'] = test_encoded['sqft'] + test_encoded['sqft_fbsmt']\n",
        "\n",
        "non_zero_lot = test_encoded.loc[test_encoded[\"sqft_lot\"] > 0, \"sqft_lot\"]\n",
        "min_val = non_zero_lot.min()\n",
        "median_val = non_zero_lot.median()\n",
        "\n",
        "test_encoded[\"sqft_lot\"] = test_encoded[\"sqft_lot\"].replace(0, median_val)\n",
        "\n",
        "\n",
        "test_encoded[\"floor_ratio\"] = np.where(\n",
        "    test_encoded[\"sqft_lot\"] == 0,\n",
        "    0,  # æˆ–å…¶ä»–æ›¿ä»£å€¼ï¼Œä¾‹å¦‚å¹³å‡å€¼\n",
        "    test_encoded[\"sqft\"] / test_encoded[\"sqft_lot\"]\n",
        ")\n",
        "\n",
        "test_encoded[\"is_large_house\"] = (test_encoded[\"sqft\"] > 3000).astype(int)\n",
        "test_encoded[\"is_recent_reno\"] = (test_encoded[\"years_since_reno\"] <= 5).astype(int)\n",
        "test_encoded[\"bath_per_bed\"] = test_encoded[\"total_baths\"] / test_encoded[\"beds\"]\n",
        "test_encoded[\"bath_per_bed\"] = test_encoded[\"bath_per_bed\"].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "# å¹´é½¡å€é–“åˆ†ç®±\n",
        "test_encoded[\"age_bin\"] = pd.cut(\n",
        "    test_encoded[\"age\"],\n",
        "    bins=[-1, 10, 30, 60, 200],\n",
        "    labels=[\"0-10\", \"11-30\", \"31-60\", \"60+\"]\n",
        ")\n",
        "age_dummies = pd.get_dummies(test_encoded[\"age_bin\"], prefix=\"age_bin\")\n",
        "test_encoded = pd.concat([test_encoded, age_dummies], axis=1)\n",
        "test_encoded.drop(columns=[\"age_bin\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_features = ['latitude', 'longitude', 'area', 'sqft', 'total_value']\n",
        "\n",
        "# æ¨™æº–åŒ–\n",
        "scaler = StandardScaler()\n",
        "test_cluster_scaled = scaler.fit_transform(test_encoded[cluster_features])\n",
        "\n",
        "# å»ºç«‹ KMeans ç¾¤çµ„ï¼ˆå»ºè­°å…ˆå¾ž 10 ç¾¤é–‹å§‹ï¼‰\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init='auto')\n",
        "test_encoded['region_cluster'] = kmeans.fit_predict(test_cluster_scaled)\n",
        "\n",
        "# One-hot ç·¨ç¢¼\n",
        "cluster_ohe = pd.get_dummies(test_encoded['region_cluster'], prefix=\"region\")\n",
        "test_encoded = pd.concat([test_encoded, cluster_ohe], axis=1)\n",
        "\n",
        "# ç§»é™¤åŽŸå§‹ cluster idï¼ˆå› ç‚ºæ˜¯é¡žåˆ¥ï¼‰\n",
        "test_encoded.drop(columns=['region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "pca_features = ['latitude', 'longitude', 'sqft', 'area', 'total_value', 'imp_val']\n",
        "\n",
        "# ðŸ”ƒ æ¨™æº–åŒ– â†’ PCA â†’ KMeans\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(test_encoded[pca_features])\n",
        "\n",
        "pca = PCA(n_components=3, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "test_encoded['pca_region_cluster'] = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# ðŸ”„ One-hot ç·¨ç¢¼\n",
        "region_dummies = pd.get_dummies(test_encoded['pca_region_cluster'], prefix='pca_region')\n",
        "test_encoded = pd.concat([test_encoded, region_dummies], axis=1)\n",
        "\n",
        "test_encoded.drop(columns=['pca_region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_encoded = clean_features(test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bool       75\n",
            "int64      35\n",
            "float64    11\n",
            "int32       6\n",
            "Name: count, dtype: int64\n",
            "bool       75\n",
            "int64      34\n",
            "float64    11\n",
            "int32       6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#ç¢ºèªè³‡æ–™é¡žåž‹\n",
        "print(train_encoded.dtypes.value_counts())\n",
        "print(test_encoded.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "has inf: False\n",
            "has NaN: False\n",
            "inf columns: []\n",
            "NaN columns: []\n"
          ]
        }
      ],
      "source": [
        "test_numeric = train_encoded.select_dtypes(include=[np.number])\n",
        "\n",
        "# æª¢æŸ¥æ˜¯å¦æœ‰ inf æˆ– NaN\n",
        "print(\"has inf:\", np.isinf(test_numeric.to_numpy()).any())\n",
        "print(\"has NaN:\", test_numeric.isnull().any().any())\n",
        "\n",
        "# çœ‹æ˜¯å“ªäº›æ¬„ä½å‡ºå•é¡Œ\n",
        "print(\"inf columns:\", test_numeric.columns[np.isinf(test_numeric.to_numpy()).any(axis=0)].tolist())\n",
        "print(\"NaN columns:\", test_numeric.columns[test_numeric.isnull().any()].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "stack model to predict the best lower and upper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LGBM Quantile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ†å‰²è¨“ç·´ç‰¹å¾µèˆ‡ç›®æ¨™\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id å¯ç•™çµ¦æœ€å¾Œè¼¸å‡º\n",
        "y = train_encoded['sale_price']\n",
        "test_encoded = test_encoded.drop(columns=['id'])\n",
        "\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_oof_preds(X, y, X_test, model_name='xgb', n_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    ç‚ºæŒ‡å®šçš„ model_name åšä¸Šä¸‹é™ OOF é æ¸¬ + test é æ¸¬\n",
        "    å‚³å›ž oof_lower, oof_upper, test_lower, test_upper å››çµ„é æ¸¬çµæžœ\n",
        "    \"\"\"\n",
        "    oof_lower = np.zeros(len(X))\n",
        "    oof_upper = np.zeros(len(X))\n",
        "    test_preds_lower = []\n",
        "    test_preds_upper = []\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        y_lower_train = y_train * 0.9\n",
        "        y_upper_train = y_train * 1.1\n",
        "\n",
        "        # æ±ºå®šæ¨¡åž‹\n",
        "        if model_name == 'xgb':\n",
        "            model_lower = xgb.XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "                                           eval_metric='rmse', tree_method='gpu_hist', random_state=fold)\n",
        "            model_upper = clone(model_lower)\n",
        "\n",
        "        elif model_name == 'lgb':\n",
        "            model_lower = lgb.LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "                                            objective='quantile', alpha=0.05, device='gpu', random_state=fold)\n",
        "            model_upper = lgb.LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "                                            objective='quantile', alpha=0.95, device='gpu', random_state=fold)\n",
        "            \n",
        "        elif model_name == 'ridge':\n",
        "            model_lower = Ridge(alpha=1.0)\n",
        "            model_upper = Ridge(alpha=1.0)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model_name: {model_name}\")\n",
        "\n",
        "        # è¨“ç·´èˆ‡é æ¸¬\n",
        "        model_lower.fit(X_train, y_lower_train)\n",
        "        model_upper.fit(X_train, y_upper_train)\n",
        "\n",
        "        oof_lower[val_idx] = model_lower.predict(X_val)\n",
        "        oof_upper[val_idx] = model_upper.predict(X_val)\n",
        "\n",
        "        test_preds_lower.append(model_lower.predict(X_test))\n",
        "        test_preds_upper.append(model_upper.predict(X_test))\n",
        "\n",
        "    # å° test é æ¸¬å¹³å‡ï¼ˆn foldï¼‰\n",
        "    test_lower = np.mean(test_preds_lower, axis=0)\n",
        "    test_upper = np.mean(test_preds_upper, axis=0)\n",
        "\n",
        "    return oof_lower, oof_upper, test_lower, test_upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4246\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.008933 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4246\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.006134 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1578500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007795 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.010286 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1573000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4241\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.006480 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4241\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007370 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1574100.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4234\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007362 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4234\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007678 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1584000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007363 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007438 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1573000.000000\n"
          ]
        }
      ],
      "source": [
        "# ç‚º XGBoost å»ºç«‹ OOF é æ¸¬\n",
        "xgb_lower_oof, xgb_upper_oof, test_xgb_lower, test_xgb_upper = get_oof_preds(X, y, test_encoded, model_name='xgb')\n",
        "\n",
        "# ç‚º LightGBM å»ºç«‹ OOF é æ¸¬\n",
        "lgb_lower_oof, lgb_upper_oof, test_lgb_lower, test_lgb_upper = get_oof_preds(X, y, test_encoded, model_name='lgb')\n",
        "\n",
        "# ç‚º Ridge å»ºç«‹ OOF é æ¸¬\n",
        "ridge_lower_oof, ridge_upper_oof, test_ridge_lower, test_ridge_upper = get_oof_preds(X, y, test_encoded, model_name='ridge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
        "    width = upper - lower\n",
        "    below = np.maximum(lower - y_true, 0)\n",
        "    above = np.maximum(y_true - upper, 0)\n",
        "    return width + (2 / alpha) * (below + above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def oof_and_hill_climb_with_meta(X_meta, y, alpha=0.1, n_splits=5, seed=42, steps=100):\n",
        "    \n",
        "\n",
        "    n = len(X_meta)\n",
        "    oof_lowers = np.zeros(n)\n",
        "    oof_uppers = np.zeros(n)\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    # ---- [1] å»ºç«‹ meta learnerï¼ˆç”¨ LGBM æˆ– Ridge éƒ½å¯ä»¥ï¼‰ ----\n",
        "    meta_lower_model = LGBMRegressor(random_state=seed, n_estimators=500, learning_rate=0.05)\n",
        "    meta_upper_model = LGBMRegressor(random_state=seed, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_meta)):\n",
        "        X_train, X_val = X_meta.iloc[train_idx], X_meta.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        y_lower_train = y_train * 0.9 - 1000\n",
        "        y_upper_train = y_train * 1.1 + 1000\n",
        "\n",
        "        lower_model = clone(meta_lower_model)\n",
        "        upper_model = clone(meta_upper_model)\n",
        "\n",
        "        lower_model.fit(X_train, y_lower_train)\n",
        "        upper_model.fit(X_train, y_upper_train)\n",
        "\n",
        "        oof_lowers[val_idx] = lower_model.predict(X_val)\n",
        "        oof_uppers[val_idx] = upper_model.predict(X_val)\n",
        "\n",
        "    # ---- [2] hill climbing å°‹æ‰¾æœ€ä½³ w1, w2 çµ„åˆ ----\n",
        "    current_w1 = 0.4\n",
        "    current_w2 = 0.6\n",
        "    best_score = np.inf\n",
        "    best_weights = (current_w1, current_w2)\n",
        "\n",
        "    for step in range(steps):\n",
        "        perturb1 = np.random.dirichlet([9])[0] - 0.9\n",
        "        perturb2 = np.random.dirichlet([9])[0] - 0.9\n",
        "\n",
        "        w1 = np.clip(current_w1 + 0.1 * perturb1, 0, 1)\n",
        "        w2 = np.clip(current_w2 + 0.1 * perturb2, 0, 1)\n",
        "\n",
        "        lower_combined = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "        upper_combined = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "\n",
        "        lower_combined, upper_combined = np.minimum(lower_combined, upper_combined), np.maximum(lower_combined, upper_combined)\n",
        "\n",
        "        score = np.mean(winkler_score(y, lower_combined, upper_combined, alpha))\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_weights = (w1, w2)\n",
        "            current_w1, current_w2 = w1, w2\n",
        "            print(f\"[Step {step}] âœ… Improved Score: {score:.2f} (w1: {w1:.4f}, w2: {w2:.4f})\")\n",
        "\n",
        "    return oof_lowers, oof_uppers, best_weights, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_meta = pd.DataFrame({\n",
        "    'xgb_lower': xgb_lower_oof,\n",
        "    'xgb_upper': xgb_upper_oof,\n",
        "    'lgb_lower': lgb_lower_oof,\n",
        "    'lgb_upper': lgb_upper_oof,\n",
        "    'ridge_lower': ridge_lower_oof,\n",
        "    'ridge_upper': ridge_upper_oof,\n",
        "\n",
        "    # å¹³å‡é æ¸¬ï¼ˆæå‡ç©©å®šæ€§ï¼‰\n",
        "    'avg_lower': (xgb_lower_oof + lgb_lower_oof + ridge_lower_oof) / 3,\n",
        "    'avg_upper': (xgb_upper_oof + lgb_upper_oof + ridge_upper_oof) / 3,\n",
        "\n",
        "    # å€é–“å¯¬åº¦ï¼ˆæ¨¡åž‹å¯èƒ½å­¸æœƒè‡ªå‹•æ‹‰å¯¬ï¼‰\n",
        "    'interval_width_xgb': xgb_upper_oof - xgb_lower_oof,\n",
        "    'interval_width_lgb': lgb_upper_oof - lgb_lower_oof,\n",
        "    'interval_width_ridge': ridge_upper_oof - ridge_lower_oof,\n",
        "\n",
        "    # æ¥µç«¯åˆ¤æ–·ï¼ˆåˆ¤æ–·æ˜¯å¦æœ‰åè½‰ï¼‰\n",
        "    'lower_gt_upper_lgb': (lgb_lower_oof > lgb_upper_oof).astype(int)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006058 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 525018.728175\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007067 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 643911.778866\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003591 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 524608.914022\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003182 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 643410.894906\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003764 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 524653.924262\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006366 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 643465.907412\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005440 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 525252.281522\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003171 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 644197.232952\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003310 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 524138.878151\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006959 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 642836.406635\n",
            "[Step 0] âœ… Improved Score: 1323304.56 (w1: 0.4100, w2: 0.6100)\n",
            "[Step 1] âœ… Improved Score: 1302388.58 (w1: 0.4200, w2: 0.6200)\n",
            "[Step 2] âœ… Improved Score: 1281779.45 (w1: 0.4300, w2: 0.6300)\n",
            "[Step 3] âœ… Improved Score: 1261484.36 (w1: 0.4400, w2: 0.6400)\n",
            "[Step 4] âœ… Improved Score: 1241494.00 (w1: 0.4500, w2: 0.6500)\n",
            "[Step 5] âœ… Improved Score: 1221807.13 (w1: 0.4600, w2: 0.6600)\n",
            "[Step 6] âœ… Improved Score: 1202423.72 (w1: 0.4700, w2: 0.6700)\n",
            "[Step 7] âœ… Improved Score: 1183353.56 (w1: 0.4800, w2: 0.6800)\n",
            "[Step 8] âœ… Improved Score: 1164591.83 (w1: 0.4900, w2: 0.6900)\n",
            "[Step 9] âœ… Improved Score: 1146135.73 (w1: 0.5000, w2: 0.7000)\n",
            "[Step 10] âœ… Improved Score: 1127982.44 (w1: 0.5100, w2: 0.7100)\n",
            "[Step 11] âœ… Improved Score: 1110137.39 (w1: 0.5200, w2: 0.7200)\n",
            "[Step 12] âœ… Improved Score: 1092593.63 (w1: 0.5300, w2: 0.7300)\n",
            "[Step 13] âœ… Improved Score: 1075347.53 (w1: 0.5400, w2: 0.7400)\n",
            "[Step 14] âœ… Improved Score: 1058390.44 (w1: 0.5500, w2: 0.7500)\n",
            "[Step 15] âœ… Improved Score: 1041725.52 (w1: 0.5600, w2: 0.7600)\n",
            "[Step 16] âœ… Improved Score: 1025360.04 (w1: 0.5700, w2: 0.7700)\n",
            "[Step 17] âœ… Improved Score: 1009286.10 (w1: 0.5800, w2: 0.7800)\n",
            "[Step 18] âœ… Improved Score: 993501.75 (w1: 0.5900, w2: 0.7900)\n",
            "[Step 19] âœ… Improved Score: 978010.50 (w1: 0.6000, w2: 0.8000)\n",
            "[Step 20] âœ… Improved Score: 962813.73 (w1: 0.6100, w2: 0.8100)\n",
            "[Step 21] âœ… Improved Score: 947912.11 (w1: 0.6200, w2: 0.8200)\n",
            "[Step 22] âœ… Improved Score: 933287.00 (w1: 0.6300, w2: 0.8300)\n",
            "[Step 23] âœ… Improved Score: 918945.36 (w1: 0.6400, w2: 0.8400)\n",
            "[Step 24] âœ… Improved Score: 904885.08 (w1: 0.6500, w2: 0.8500)\n",
            "[Step 25] âœ… Improved Score: 891092.50 (w1: 0.6600, w2: 0.8600)\n",
            "[Step 26] âœ… Improved Score: 877578.03 (w1: 0.6700, w2: 0.8700)\n",
            "[Step 27] âœ… Improved Score: 864342.03 (w1: 0.6800, w2: 0.8800)\n",
            "[Step 28] âœ… Improved Score: 851375.72 (w1: 0.6900, w2: 0.8900)\n",
            "[Step 29] âœ… Improved Score: 838682.66 (w1: 0.7000, w2: 0.9000)\n",
            "[Step 30] âœ… Improved Score: 826259.29 (w1: 0.7100, w2: 0.9100)\n",
            "[Step 31] âœ… Improved Score: 814102.31 (w1: 0.7200, w2: 0.9200)\n",
            "[Step 32] âœ… Improved Score: 802203.71 (w1: 0.7300, w2: 0.9300)\n",
            "[Step 33] âœ… Improved Score: 790572.07 (w1: 0.7400, w2: 0.9400)\n",
            "[Step 34] âœ… Improved Score: 779193.92 (w1: 0.7500, w2: 0.9500)\n",
            "[Step 35] âœ… Improved Score: 768063.88 (w1: 0.7600, w2: 0.9600)\n",
            "[Step 36] âœ… Improved Score: 757180.54 (w1: 0.7700, w2: 0.9700)\n",
            "[Step 37] âœ… Improved Score: 746542.95 (w1: 0.7800, w2: 0.9800)\n",
            "[Step 38] âœ… Improved Score: 736155.22 (w1: 0.7900, w2: 0.9900)\n",
            "[Step 39] âœ… Improved Score: 726013.09 (w1: 0.8000, w2: 1.0000)\n",
            "[Step 40] âœ… Improved Score: 719601.90 (w1: 0.8100, w2: 1.0000)\n",
            "[Step 41] âœ… Improved Score: 713338.21 (w1: 0.8200, w2: 1.0000)\n",
            "[Step 42] âœ… Improved Score: 707212.19 (w1: 0.8300, w2: 1.0000)\n",
            "[Step 43] âœ… Improved Score: 701233.06 (w1: 0.8400, w2: 1.0000)\n",
            "[Step 44] âœ… Improved Score: 695405.13 (w1: 0.8500, w2: 1.0000)\n",
            "[Step 45] âœ… Improved Score: 689723.05 (w1: 0.8600, w2: 1.0000)\n",
            "[Step 46] âœ… Improved Score: 684176.32 (w1: 0.8700, w2: 1.0000)\n",
            "[Step 47] âœ… Improved Score: 678761.51 (w1: 0.8800, w2: 1.0000)\n",
            "[Step 48] âœ… Improved Score: 673476.43 (w1: 0.8900, w2: 1.0000)\n",
            "[Step 49] âœ… Improved Score: 668326.40 (w1: 0.9000, w2: 1.0000)\n",
            "[Step 50] âœ… Improved Score: 663305.77 (w1: 0.9100, w2: 1.0000)\n",
            "[Step 51] âœ… Improved Score: 658414.37 (w1: 0.9200, w2: 1.0000)\n",
            "[Step 52] âœ… Improved Score: 653648.03 (w1: 0.9300, w2: 1.0000)\n",
            "[Step 53] âœ… Improved Score: 649001.62 (w1: 0.9400, w2: 1.0000)\n",
            "[Step 54] âœ… Improved Score: 644473.39 (w1: 0.9500, w2: 1.0000)\n",
            "[Step 55] âœ… Improved Score: 640062.74 (w1: 0.9600, w2: 1.0000)\n",
            "[Step 56] âœ… Improved Score: 635772.24 (w1: 0.9700, w2: 1.0000)\n",
            "[Step 57] âœ… Improved Score: 631604.80 (w1: 0.9800, w2: 1.0000)\n",
            "[Step 58] âœ… Improved Score: 627558.45 (w1: 0.9900, w2: 1.0000)\n",
            "[Step 59] âœ… Improved Score: 623629.41 (w1: 1.0000, w2: 1.0000)\n"
          ]
        }
      ],
      "source": [
        "oof_lowers, oof_uppers, best_weight, best_score = oof_and_hill_climb_with_meta(X_meta, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "before plus floor ratio~bath per bed score = 341636.83\n",
        "after plus new feature score = 342013\n",
        "after adjust model = 387816"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "è¼¸å‡ºtest LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#å¡«è£œç¼ºæ¼æ¬„ä½ï¼ˆå°é½Šè¨“ç·´é›†æ¬„ä½ï¼‰\n",
        "missing_cols = set(X.columns) - set(test_encoded.columns)\n",
        "for col in missing_cols:\n",
        "    test_encoded[col] = 0\n",
        "\n",
        "# ç¢ºä¿æ¬„ä½é †åºä¸€è‡´\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.005452 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004503 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1435000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004989 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004292 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004324 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003735 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1431000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004233 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003492 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1440000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004044 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003498 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[Step 0] Improved MWIS: 6132346.42 (Weight: 0.4100)\n",
            "[Step 1] Improved MWIS: 5998138.12 (Weight: 0.4200)\n",
            "[Step 2] Improved MWIS: 5864083.19 (Weight: 0.4300)\n",
            "[Step 3] Improved MWIS: 5730199.30 (Weight: 0.4400)\n",
            "[Step 4] Improved MWIS: 5596511.15 (Weight: 0.4500)\n",
            "[Step 5] Improved MWIS: 5463018.77 (Weight: 0.4600)\n",
            "[Step 6] Improved MWIS: 5329716.49 (Weight: 0.4700)\n",
            "[Step 7] Improved MWIS: 5196602.32 (Weight: 0.4800)\n",
            "[Step 8] Improved MWIS: 5063747.28 (Weight: 0.4900)\n",
            "[Step 9] Improved MWIS: 4931168.66 (Weight: 0.5000)\n",
            "[Step 10] Improved MWIS: 4798872.28 (Weight: 0.5100)\n",
            "[Step 11] Improved MWIS: 4666890.87 (Weight: 0.5200)\n",
            "[Step 12] Improved MWIS: 4535241.47 (Weight: 0.5300)\n",
            "[Step 13] Improved MWIS: 4403949.01 (Weight: 0.5400)\n",
            "[Step 14] Improved MWIS: 4273021.68 (Weight: 0.5500)\n",
            "[Step 15] Improved MWIS: 4142466.76 (Weight: 0.5600)\n",
            "[Step 16] Improved MWIS: 4012315.06 (Weight: 0.5700)\n",
            "[Step 17] Improved MWIS: 3882623.63 (Weight: 0.5800)\n",
            "[Step 18] Improved MWIS: 3753424.52 (Weight: 0.5900)\n",
            "[Step 19] Improved MWIS: 3624740.01 (Weight: 0.6000)\n",
            "[Step 20] Improved MWIS: 3496604.74 (Weight: 0.6100)\n",
            "[Step 21] Improved MWIS: 3369087.47 (Weight: 0.6200)\n",
            "[Step 22] Improved MWIS: 3242267.39 (Weight: 0.6300)\n",
            "[Step 23] Improved MWIS: 3116209.35 (Weight: 0.6400)\n",
            "[Step 24] Improved MWIS: 2991022.61 (Weight: 0.6500)\n",
            "[Step 25] Improved MWIS: 2866738.68 (Weight: 0.6600)\n",
            "[Step 26] Improved MWIS: 2743466.81 (Weight: 0.6700)\n",
            "[Step 27] Improved MWIS: 2621243.19 (Weight: 0.6800)\n",
            "[Step 28] Improved MWIS: 2500209.55 (Weight: 0.6900)\n",
            "[Step 29] Improved MWIS: 2380487.12 (Weight: 0.7000)\n",
            "[Step 30] Improved MWIS: 2262178.94 (Weight: 0.7100)\n",
            "[Step 31] Improved MWIS: 2145526.51 (Weight: 0.7200)\n",
            "[Step 32] Improved MWIS: 2030689.68 (Weight: 0.7300)\n",
            "[Step 33] Improved MWIS: 1917840.75 (Weight: 0.7400)\n",
            "[Step 34] Improved MWIS: 1807139.16 (Weight: 0.7500)\n",
            "[Step 35] Improved MWIS: 1698786.24 (Weight: 0.7600)\n",
            "[Step 36] Improved MWIS: 1593036.82 (Weight: 0.7700)\n",
            "[Step 37] Improved MWIS: 1490133.56 (Weight: 0.7800)\n",
            "[Step 38] Improved MWIS: 1390254.94 (Weight: 0.7900)\n",
            "[Step 39] Improved MWIS: 1293675.49 (Weight: 0.8000)\n",
            "[Step 40] Improved MWIS: 1200790.81 (Weight: 0.8100)\n",
            "[Step 41] Improved MWIS: 1111796.51 (Weight: 0.8200)\n",
            "[Step 42] Improved MWIS: 1026969.04 (Weight: 0.8300)\n",
            "[Step 43] Improved MWIS: 946610.10 (Weight: 0.8400)\n",
            "[Step 44] Improved MWIS: 870883.14 (Weight: 0.8500)\n",
            "[Step 45] Improved MWIS: 800085.84 (Weight: 0.8600)\n",
            "[Step 46] Improved MWIS: 734358.53 (Weight: 0.8700)\n",
            "[Step 47] Improved MWIS: 673869.19 (Weight: 0.8800)\n",
            "[Step 48] Improved MWIS: 618718.30 (Weight: 0.8900)\n",
            "[Step 49] Improved MWIS: 569060.83 (Weight: 0.9000)\n",
            "[Step 50] Improved MWIS: 524864.35 (Weight: 0.9100)\n",
            "[Step 51] Improved MWIS: 486048.75 (Weight: 0.9200)\n",
            "[Step 52] Improved MWIS: 452383.38 (Weight: 0.9300)\n",
            "[Step 53] Improved MWIS: 423677.89 (Weight: 0.9400)\n",
            "[Step 54] Improved MWIS: 399765.80 (Weight: 0.9500)\n",
            "[Step 55] Improved MWIS: 380256.93 (Weight: 0.9600)\n",
            "[Step 56] Improved MWIS: 364930.36 (Weight: 0.9700)\n",
            "[Step 57] Improved MWIS: 353482.91 (Weight: 0.9800)\n",
            "[Step 58] Improved MWIS: 345674.64 (Weight: 0.9900)\n",
            "[Step 59] Improved MWIS: 341379.28 (Weight: 1.0000)\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3856\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (5.34 MB) transferred to GPU in 0.005645 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3856\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (5.34 MB) transferred to GPU in 0.004793 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1434006.000000\n"
          ]
        }
      ],
      "source": [
        "model_lower = models[\"lower\"]\n",
        "model_upper = models[\"upper\"]\n",
        "\n",
        "final_model_lower = clone(model_lower).fit(X, y)\n",
        "final_model_upper = clone(model_upper).fit(X, y)\n",
        "\n",
        "'''test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = best_weight * test_lower\n",
        "final_upper = best_weight * test_upper\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)'''\n",
        "\n",
        "\n",
        "test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = w1 * test_lower + (1 - w1) * test_upper\n",
        "final_upper = w2 * test_upper + (1 - w2) * test_lower\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)  # optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "submission_df.head()\n",
        "test_encoded['id'] = test_df['id']  # é€™è¡Œå…ˆè£œä¸Š id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       id       pi_lower      pi_upper\n",
            "0  200000  823500.777056  1.125383e+06\n",
            "1  200001  590158.900174  7.270068e+05\n",
            "2  200002  469229.953050  6.502398e+05\n",
            "3  200003  315274.697956  4.363184e+05\n",
            "4  200004  413919.532224  6.498657e+05\n"
          ]
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'id': test_encoded['id'],  # å¿…é ˆèˆ‡ sample_submission å°é½Š\n",
        "    'pi_lower': final_lower,\n",
        "    'pi_upper': final_upper\n",
        "})\n",
        "\n",
        "# è¼¸å‡ºæˆ CSV\n",
        "submission_df.to_csv('lgbm_predict.csv', index=False)\n",
        "print(submission_df.head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_tensor_evn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
