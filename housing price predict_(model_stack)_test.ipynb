{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "train_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\dataset.csv\"\n",
        "test_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\test.csv\"\n",
        "sam_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\sample_submission.csv\"\n",
        "\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000, 47)\n",
            "Index(['id', 'sale_date', 'sale_price', 'sale_nbr', 'sale_warning',\n",
            "       'join_status', 'join_year', 'latitude', 'longitude', 'area', 'city',\n",
            "       'zoning', 'subdivision', 'present_use', 'land_val', 'imp_val',\n",
            "       'year_built', 'year_reno', 'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
            "       'grade', 'fbsmt_grade', 'condition', 'stories', 'beds', 'bath_full',\n",
            "       'bath_3qtr', 'bath_half', 'garb_sqft', 'gara_sqft', 'wfnt', 'golf',\n",
            "       'greenbelt', 'noise_traffic', 'view_rainier', 'view_olympics',\n",
            "       'view_cascades', 'view_territorial', 'view_skyline', 'view_sound',\n",
            "       'view_lakewash', 'view_lakesamm', 'view_otherwater', 'view_other',\n",
            "       'submarket'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sale_date</th>\n",
              "      <th>sale_price</th>\n",
              "      <th>sale_nbr</th>\n",
              "      <th>sale_warning</th>\n",
              "      <th>join_status</th>\n",
              "      <th>join_year</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>area</th>\n",
              "      <th>...</th>\n",
              "      <th>view_olympics</th>\n",
              "      <th>view_cascades</th>\n",
              "      <th>view_territorial</th>\n",
              "      <th>view_skyline</th>\n",
              "      <th>view_sound</th>\n",
              "      <th>view_lakewash</th>\n",
              "      <th>view_lakesamm</th>\n",
              "      <th>view_otherwater</th>\n",
              "      <th>view_other</th>\n",
              "      <th>submarket</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-15</td>\n",
              "      <td>236000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.2917</td>\n",
              "      <td>-122.3658</td>\n",
              "      <td>53</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1999-01-15</td>\n",
              "      <td>313300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.6531</td>\n",
              "      <td>-122.1996</td>\n",
              "      <td>74</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2006-08-15</td>\n",
              "      <td>341000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4733</td>\n",
              "      <td>-122.1901</td>\n",
              "      <td>30</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1999-12-15</td>\n",
              "      <td>267000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4739</td>\n",
              "      <td>-122.3295</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-15</td>\n",
              "      <td>1650000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>miss99</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.7516</td>\n",
              "      <td>-122.1222</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 47 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id   sale_date  sale_price  sale_nbr sale_warning join_status  join_year  \\\n",
              "0   0  2014-11-15      236000       2.0                    nochg       2025   \n",
              "1   1  1999-01-15      313300       NaN          26        nochg       2025   \n",
              "2   2  2006-08-15      341000       1.0                    nochg       2025   \n",
              "3   3  1999-12-15      267000       1.0                    nochg       2025   \n",
              "4   4  2018-07-15     1650000       2.0                   miss99       2025   \n",
              "\n",
              "   latitude  longitude  area  ... view_olympics view_cascades  \\\n",
              "0   47.2917  -122.3658    53  ...             0             0   \n",
              "1   47.6531  -122.1996    74  ...             0             0   \n",
              "2   47.4733  -122.1901    30  ...             0             0   \n",
              "3   47.4739  -122.3295    96  ...             0             0   \n",
              "4   47.7516  -122.1222    36  ...             0             0   \n",
              "\n",
              "  view_territorial  view_skyline  view_sound  view_lakewash  view_lakesamm  \\\n",
              "0                0             0           0              0              0   \n",
              "1                0             0           0              1              0   \n",
              "2                0             0           0              0              0   \n",
              "3                0             0           0              0              0   \n",
              "4                0             0           0              0              0   \n",
              "\n",
              "   view_otherwater  view_other  submarket  \n",
              "0                0           0          I  \n",
              "1                0           0          Q  \n",
              "2                0           0          K  \n",
              "3                0           0          G  \n",
              "4                0           0          P  \n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "sample_df = pd.read_csv(sam_path)\n",
        "\n",
        "print(train_df.shape)\n",
        "print(train_df.columns)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sale_nbr       42182\n",
              "subdivision    17550\n",
              "submarket       1717\n",
              "dtype: int64"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#計算缺失值\n",
        "missing_values = train_df.isnull().sum()\n",
        "missing_values[missing_values > 0].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 train_encoded\n",
        "processed_cols = []\n",
        "train_encoded = pd.DataFrame()\n",
        "\n",
        "# One-hot \n",
        "confirmed_one_hot = [\n",
        "    'join_status', 'condition', 'stories', 'grade', 'fbsmt_grade', 'present_use'\n",
        "]\n",
        "onehot_df = pd.get_dummies(train_df[confirmed_one_hot], drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, onehot_df], axis=1)\n",
        "processed_cols += confirmed_one_hot\n",
        "\n",
        "\n",
        "train_df['sale_date'] = pd.to_datetime(train_df['sale_date'], errors='coerce')\n",
        "train_encoded['sale_year'] = train_df['sale_date'].dt.year\n",
        "train_encoded['sale_month'] = train_df['sale_date'].dt.month\n",
        "train_encoded['sale_season'] = ((train_encoded['sale_month'] - 1) // 3 + 1)\n",
        "processed_cols += ['sale_date']\n",
        "\n",
        "# 原始數值直接加入\n",
        "direct_add_cols = [\n",
        "    'id', 'sale_price', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half', 'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'  to much missing value\n",
        "]\n",
        "for col in direct_add_cols:\n",
        "    train_encoded[col] = train_df[col]\n",
        "processed_cols += direct_add_cols\n",
        "\n",
        "# 統整城市、市場與銷售警告資訊\n",
        "top_cities = train_df['city'].value_counts().nlargest(10).index.tolist()\n",
        "top_supermarket = train_df['submarket'].value_counts().nlargest(10).index.tolist()\n",
        "top_sale_warning = train_df['sale_warning'].value_counts().nlargest(15).index.tolist()\n",
        "\n",
        "train_encoded['city_simplified'] = train_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "train_encoded['submarket_simplified'] = train_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "train_encoded['sale_warning_simplified'] = train_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "\n",
        "city_dummy = pd.get_dummies(train_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "submarket_dummy = pd.get_dummies(train_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "sale_warning_dummy = pd.get_dummies(train_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, city_dummy, submarket_dummy, sale_warning_dummy], axis=1)\n",
        "processed_cols += ['city', 'submarket', 'sale_warning']\n",
        "\n",
        "# Zoning 群組分類\n",
        "def zoning_group_classify(z):\n",
        "    if pd.isna(z): return 'other'\n",
        "    z = z.upper()\n",
        "    if 'SF' in z: return 'SF'\n",
        "    elif 'MR' in z: return 'MR'\n",
        "    elif 'NC' in z: return 'NC'\n",
        "    elif 'HR' in z or 'IG' in z: return 'other'\n",
        "    elif 'P' in z: return 'P'\n",
        "    return 'other'\n",
        "\n",
        "train_encoded['zoning_group'] = train_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(train_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, zoning_dummy], axis=1)\n",
        "train_encoded.drop(columns=['zoning_group'], inplace=True)\n",
        "processed_cols += ['zoning']\n",
        "\n",
        "\n",
        "# 碎片化資訊統整成新欄位\n",
        "train_encoded['age'] = train_encoded['sale_year'] - train_encoded['year_built']\n",
        "train_encoded['renovated'] = np.where(train_encoded['year_reno'] > 0, 1, 0)\n",
        "train_encoded['years_since_reno'] = np.where(train_encoded['renovated'], train_encoded['sale_year'] - train_encoded['year_reno'], 0)\n",
        "train_encoded['total_baths'] = train_encoded['bath_full'] + 0.75 * train_encoded['bath_3qtr'] + 0.5 * train_encoded['bath_half']\n",
        "train_encoded['total_value'] = train_encoded['land_val'] + train_encoded['imp_val']\n",
        "train_encoded['living_area'] = train_encoded['sqft'] + train_encoded['sqft_fbsmt']\n",
        "\n",
        "# 刪除用完的簡化文字類欄位\n",
        "for col in ['city_simplified', 'submarket_simplified', 'sale_warning_simplified']:\n",
        "    train_encoded.drop(columns=[col], inplace=True)\n",
        "\n",
        "\n",
        "#新增特徵\n",
        "non_zero_lot = train_encoded.loc[train_encoded[\"sqft_lot\"] > 0, \"sqft_lot\"]\n",
        "min_val = non_zero_lot.min()\n",
        "median_val = non_zero_lot.median()\n",
        "\n",
        "train_encoded[\"sqft_lot\"] = train_encoded[\"sqft_lot\"].replace(0, median_val)\n",
        "\n",
        "#新增特徵\n",
        "train_encoded[\"floor_ratio\"] = np.where(\n",
        "    train_encoded[\"sqft_lot\"] == 0,\n",
        "    0,\n",
        "    train_encoded[\"sqft\"] / train_encoded[\"sqft_lot\"]\n",
        ")\n",
        "\n",
        "train_encoded[\"is_large_house\"] = (train_encoded[\"sqft\"] > 3000).astype(int)\n",
        "train_encoded[\"is_recent_reno\"] = (train_encoded[\"years_since_reno\"] <= 5).astype(int)\n",
        "train_encoded[\"bath_per_bed\"] = train_encoded[\"total_baths\"] / train_encoded[\"beds\"]\n",
        "train_encoded[\"bath_per_bed\"] = train_encoded[\"bath_per_bed\"].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "# 屋齡區間\n",
        "train_encoded[\"age_bin\"] = pd.cut(\n",
        "    train_encoded[\"age\"],\n",
        "    bins=[-1, 10, 30, 60, 200],\n",
        "    labels=[\"0-10\", \"11-30\", \"31-60\", \"60+\"]\n",
        ")\n",
        "age_dummies = pd.get_dummies(train_encoded[\"age_bin\"], prefix=\"age_bin\")\n",
        "train_encoded = pd.concat([train_encoded, age_dummies], axis=1)\n",
        "train_encoded.drop(columns=[\"age_bin\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_features = ['latitude', 'longitude', 'area', 'sqft', 'total_value']\n",
        "\n",
        "# 標準化\n",
        "scaler = StandardScaler()\n",
        "train_cluster_scaled = scaler.fit_transform(train_encoded[cluster_features])\n",
        "\n",
        "# 建立 KMeans 群組（建議先從 10 群開始）\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init='auto')\n",
        "train_encoded['region_cluster'] = kmeans.fit_predict(train_cluster_scaled)\n",
        "\n",
        "# One-hot 編碼\n",
        "cluster_ohe = pd.get_dummies(train_encoded['region_cluster'], prefix=\"region\")\n",
        "train_encoded = pd.concat([train_encoded, cluster_ohe], axis=1)\n",
        "\n",
        "# 移除原始 cluster id（因為是類別）\n",
        "train_encoded.drop(columns=['region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "pca_features = ['latitude', 'longitude', 'sqft', 'area', 'total_value', 'imp_val']\n",
        "\n",
        "# 🔃 標準化 → PCA → KMeans\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(train_encoded[pca_features])\n",
        "\n",
        "pca = PCA(n_components=3, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "train_encoded['pca_region_cluster'] = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 🔄 One-hot 編碼\n",
        "region_dummies = pd.get_dummies(train_encoded['pca_region_cluster'], prefix='pca_region')\n",
        "train_encoded = pd.concat([train_encoded, region_dummies], axis=1)\n",
        "\n",
        "train_encoded.drop(columns=['pca_region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_features(df):\n",
        "    import numpy as np\n",
        "\n",
        "    # 建議 log1p 處理（避免極端偏態影響模型）\n",
        "    log_cols = ['land_val', 'imp_val', 'sqft_lot', 'garb_sqft', 'floor_ratio', 'total_value']\n",
        "    for col in log_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.log1p(df[col])\n",
        "\n",
        "    # clip 上限值（可選，如果你不 log）\n",
        "    clip_cols = ['land_val', 'imp_val', 'sqft_lot']\n",
        "    for col in clip_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].clip(upper=1_000_000)\n",
        "            \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_encoded = clean_features(train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "direct_add_cols = [\n",
        "    'id', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half',\n",
        "    'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'沒有做這個 用意不大\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 test_encoded 空表\n",
        "test_encoded = pd.DataFrame()\n",
        "\n",
        "# 1. One-hot 欄位\n",
        "test_onehot_df = pd.get_dummies(test_df[confirmed_one_hot], drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, test_onehot_df], axis=1)\n",
        "\n",
        "# 2. 日期處理\n",
        "test_df['sale_date'] = pd.to_datetime(test_df['sale_date'], errors='coerce')\n",
        "test_encoded['sale_year'] = test_df['sale_date'].dt.year\n",
        "test_encoded['sale_month'] = test_df['sale_date'].dt.month\n",
        "test_encoded['sale_season'] = ((test_encoded['sale_month'] - 1) // 3 + 1)\n",
        "\n",
        "# 3. 加入 direct_add_cols 欄位\n",
        "for col in direct_add_cols:\n",
        "    test_encoded[col] = test_df[col]\n",
        "\n",
        "# 4. city / submarket / sale_warning (simplified)\n",
        "test_encoded['city_simplified'] = test_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "city_dummy = pd.get_dummies(test_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, city_dummy], axis=1)\n",
        "\n",
        "test_encoded['submarket_simplified'] = test_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "submarket_dummy = pd.get_dummies(test_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, submarket_dummy], axis=1)\n",
        "\n",
        "test_encoded['sale_warning_simplified'] = test_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "sale_warning_dummy = pd.get_dummies(test_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, sale_warning_dummy], axis=1)\n",
        "\n",
        "# 5. Zoning 分群 One-hot\n",
        "test_encoded['zoning_group'] = test_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(test_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, zoning_dummy], axis=1)\n",
        "test_encoded.drop(columns=['zoning_group', 'city_simplified', 'submarket_simplified', 'sale_warning_simplified'], inplace=True)\n",
        "\n",
        "\n",
        "#碎片化資訊統整成新欄位\n",
        "test_encoded['age'] = test_encoded['sale_year'] - test_encoded['year_built']\n",
        "test_encoded['renovated'] = np.where(test_encoded['year_reno'] > 0, 1, 0)\n",
        "test_encoded['years_since_reno'] = np.where(test_encoded['renovated'], test_encoded['sale_year'] - test_encoded['year_reno'], 0)\n",
        "test_encoded['total_baths'] = test_encoded['bath_full'] + 0.75 * test_encoded['bath_3qtr'] + 0.5 * test_encoded['bath_half']\n",
        "test_encoded['total_value'] = test_encoded['land_val'] + test_encoded['imp_val']\n",
        "test_encoded['living_area'] = test_encoded['sqft'] + test_encoded['sqft_fbsmt']\n",
        "\n",
        "non_zero_lot = test_encoded.loc[test_encoded[\"sqft_lot\"] > 0, \"sqft_lot\"]\n",
        "min_val = non_zero_lot.min()\n",
        "median_val = non_zero_lot.median()\n",
        "\n",
        "test_encoded[\"sqft_lot\"] = test_encoded[\"sqft_lot\"].replace(0, median_val)\n",
        "\n",
        "\n",
        "test_encoded[\"floor_ratio\"] = np.where(\n",
        "    test_encoded[\"sqft_lot\"] == 0,\n",
        "    0,  # 或其他替代值，例如平均值\n",
        "    test_encoded[\"sqft\"] / test_encoded[\"sqft_lot\"]\n",
        ")\n",
        "\n",
        "test_encoded[\"is_large_house\"] = (test_encoded[\"sqft\"] > 3000).astype(int)\n",
        "test_encoded[\"is_recent_reno\"] = (test_encoded[\"years_since_reno\"] <= 5).astype(int)\n",
        "test_encoded[\"bath_per_bed\"] = test_encoded[\"total_baths\"] / test_encoded[\"beds\"]\n",
        "test_encoded[\"bath_per_bed\"] = test_encoded[\"bath_per_bed\"].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "\n",
        "# 年齡區間分箱\n",
        "test_encoded[\"age_bin\"] = pd.cut(\n",
        "    test_encoded[\"age\"],\n",
        "    bins=[-1, 10, 30, 60, 200],\n",
        "    labels=[\"0-10\", \"11-30\", \"31-60\", \"60+\"]\n",
        ")\n",
        "age_dummies = pd.get_dummies(test_encoded[\"age_bin\"], prefix=\"age_bin\")\n",
        "test_encoded = pd.concat([test_encoded, age_dummies], axis=1)\n",
        "test_encoded.drop(columns=[\"age_bin\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_features = ['latitude', 'longitude', 'area', 'sqft', 'total_value']\n",
        "\n",
        "# 標準化\n",
        "scaler = StandardScaler()\n",
        "test_cluster_scaled = scaler.fit_transform(test_encoded[cluster_features])\n",
        "\n",
        "# 建立 KMeans 群組（建議先從 10 群開始）\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init='auto')\n",
        "test_encoded['region_cluster'] = kmeans.fit_predict(test_cluster_scaled)\n",
        "\n",
        "# One-hot 編碼\n",
        "cluster_ohe = pd.get_dummies(test_encoded['region_cluster'], prefix=\"region\")\n",
        "test_encoded = pd.concat([test_encoded, cluster_ohe], axis=1)\n",
        "\n",
        "# 移除原始 cluster id（因為是類別）\n",
        "test_encoded.drop(columns=['region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "pca_features = ['latitude', 'longitude', 'sqft', 'area', 'total_value', 'imp_val']\n",
        "\n",
        "# 🔃 標準化 → PCA → KMeans\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(test_encoded[pca_features])\n",
        "\n",
        "pca = PCA(n_components=3, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "test_encoded['pca_region_cluster'] = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 🔄 One-hot 編碼\n",
        "region_dummies = pd.get_dummies(test_encoded['pca_region_cluster'], prefix='pca_region')\n",
        "test_encoded = pd.concat([test_encoded, region_dummies], axis=1)\n",
        "\n",
        "test_encoded.drop(columns=['pca_region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_encoded = clean_features(test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bool       75\n",
            "int64      35\n",
            "float64    11\n",
            "int32       6\n",
            "Name: count, dtype: int64\n",
            "bool       75\n",
            "int64      34\n",
            "float64    11\n",
            "int32       6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#確認資料類型\n",
        "print(train_encoded.dtypes.value_counts())\n",
        "print(test_encoded.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "has inf: False\n",
            "has NaN: False\n",
            "inf columns: []\n",
            "NaN columns: []\n"
          ]
        }
      ],
      "source": [
        "test_numeric = train_encoded.select_dtypes(include=[np.number])\n",
        "\n",
        "# 檢查是否有 inf 或 NaN\n",
        "print(\"has inf:\", np.isinf(test_numeric.to_numpy()).any())\n",
        "print(\"has NaN:\", test_numeric.isnull().any().any())\n",
        "\n",
        "# 看是哪些欄位出問題\n",
        "print(\"inf columns:\", test_numeric.columns[np.isinf(test_numeric.to_numpy()).any(axis=0)].tolist())\n",
        "print(\"NaN columns:\", test_numeric.columns[test_numeric.isnull().any()].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "stack model to predict the best lower and upper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LGBM Quantile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']\n",
        "test_encoded = test_encoded.drop(columns=['id'])\n",
        "\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_oof_preds(X, y, X_test, model_name='xgb', n_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    為指定的 model_name 做上下限 OOF 預測 + test 預測\n",
        "    傳回 oof_lower, oof_upper, test_lower, test_upper 四組預測結果\n",
        "    \"\"\"\n",
        "    oof_lower = np.zeros(len(X))\n",
        "    oof_upper = np.zeros(len(X))\n",
        "    test_preds_lower = []\n",
        "    test_preds_upper = []\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        y_lower_train = y_train * 0.9\n",
        "        y_upper_train = y_train * 1.1\n",
        "\n",
        "        # 決定模型\n",
        "        if model_name == 'xgb':\n",
        "            model_lower = xgb.XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "                                           eval_metric='rmse', tree_method='gpu_hist', random_state=fold)\n",
        "            model_upper = clone(model_lower)\n",
        "\n",
        "        elif model_name == 'lgb':\n",
        "            model_lower = lgb.LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "                                            objective='quantile', alpha=0.05, device='gpu', random_state=fold)\n",
        "            model_upper = lgb.LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "                                            objective='quantile', alpha=0.95, device='gpu', random_state=fold)\n",
        "            \n",
        "        elif model_name == 'ridge':\n",
        "            model_lower = Ridge(alpha=1.0)\n",
        "            model_upper = Ridge(alpha=1.0)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model_name: {model_name}\")\n",
        "\n",
        "        # 訓練與預測\n",
        "        model_lower.fit(X_train, y_lower_train)\n",
        "        model_upper.fit(X_train, y_upper_train)\n",
        "\n",
        "        oof_lower[val_idx] = model_lower.predict(X_val)\n",
        "        oof_upper[val_idx] = model_upper.predict(X_val)\n",
        "\n",
        "        test_preds_lower.append(model_lower.predict(X_test))\n",
        "        test_preds_upper.append(model_upper.predict(X_test))\n",
        "\n",
        "    # 對 test 預測平均（n fold）\n",
        "    test_lower = np.mean(test_preds_lower, axis=0)\n",
        "    test_upper = np.mean(test_preds_upper, axis=0)\n",
        "\n",
        "    return oof_lower, oof_upper, test_lower, test_upper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:52:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:53:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4246\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.008933 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4246\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.006134 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1578500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007795 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.010286 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1573000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4241\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.006480 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4241\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007370 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1574100.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4234\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007362 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4234\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007678 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1584000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007363 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 166500.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4236\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 125\n",
            "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 32 dense feature groups (4.88 MB) transferred to GPU in 0.007438 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1573000.000000\n"
          ]
        }
      ],
      "source": [
        "# 為 XGBoost 建立 OOF 預測\n",
        "xgb_lower_oof, xgb_upper_oof, test_xgb_lower, test_xgb_upper = get_oof_preds(X, y, test_encoded, model_name='xgb')\n",
        "\n",
        "# 為 LightGBM 建立 OOF 預測\n",
        "lgb_lower_oof, lgb_upper_oof, test_lgb_lower, test_lgb_upper = get_oof_preds(X, y, test_encoded, model_name='lgb')\n",
        "\n",
        "# 為 Ridge 建立 OOF 預測\n",
        "ridge_lower_oof, ridge_upper_oof, test_ridge_lower, test_ridge_upper = get_oof_preds(X, y, test_encoded, model_name='ridge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
        "    width = upper - lower\n",
        "    below = np.maximum(lower - y_true, 0)\n",
        "    above = np.maximum(y_true - upper, 0)\n",
        "    return width + (2 / alpha) * (below + above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def oof_and_hill_climb_with_meta(X_meta, y, alpha=0.1, n_splits=5, seed=42, steps=100):\n",
        "    \n",
        "\n",
        "    n = len(X_meta)\n",
        "    oof_lowers = np.zeros(n)\n",
        "    oof_uppers = np.zeros(n)\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    # ---- [1] 建立 meta learner（用 LGBM 或 Ridge 都可以） ----\n",
        "    meta_lower_model = LGBMRegressor(random_state=seed, n_estimators=500, learning_rate=0.05)\n",
        "    meta_upper_model = LGBMRegressor(random_state=seed, n_estimators=500, learning_rate=0.05)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_meta)):\n",
        "        X_train, X_val = X_meta.iloc[train_idx], X_meta.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        y_lower_train = y_train * 0.9 - 1000\n",
        "        y_upper_train = y_train * 1.1 + 1000\n",
        "\n",
        "        lower_model = clone(meta_lower_model)\n",
        "        upper_model = clone(meta_upper_model)\n",
        "\n",
        "        lower_model.fit(X_train, y_lower_train)\n",
        "        upper_model.fit(X_train, y_upper_train)\n",
        "\n",
        "        oof_lowers[val_idx] = lower_model.predict(X_val)\n",
        "        oof_uppers[val_idx] = upper_model.predict(X_val)\n",
        "\n",
        "    # ---- [2] hill climbing 尋找最佳 w1, w2 組合 ----\n",
        "    current_w1 = 0.4\n",
        "    current_w2 = 0.6\n",
        "    best_score = np.inf\n",
        "    best_weights = (current_w1, current_w2)\n",
        "\n",
        "    for step in range(steps):\n",
        "        perturb1 = np.random.dirichlet([9])[0] - 0.9\n",
        "        perturb2 = np.random.dirichlet([9])[0] - 0.9\n",
        "\n",
        "        w1 = np.clip(current_w1 + 0.1 * perturb1, 0, 1)\n",
        "        w2 = np.clip(current_w2 + 0.1 * perturb2, 0, 1)\n",
        "\n",
        "        lower_combined = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "        upper_combined = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "\n",
        "        lower_combined, upper_combined = np.minimum(lower_combined, upper_combined), np.maximum(lower_combined, upper_combined)\n",
        "\n",
        "        score = np.mean(winkler_score(y, lower_combined, upper_combined, alpha))\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_weights = (w1, w2)\n",
        "            current_w1, current_w2 = w1, w2\n",
        "            print(f\"[Step {step}] ✅ Improved Score: {score:.2f} (w1: {w1:.4f}, w2: {w2:.4f})\")\n",
        "\n",
        "    return oof_lowers, oof_uppers, best_weights, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_meta = pd.DataFrame({\n",
        "    'xgb_lower': xgb_lower_oof,\n",
        "    'xgb_upper': xgb_upper_oof,\n",
        "    'lgb_lower': lgb_lower_oof,\n",
        "    'lgb_upper': lgb_upper_oof,\n",
        "    'ridge_lower': ridge_lower_oof,\n",
        "    'ridge_upper': ridge_upper_oof,\n",
        "\n",
        "    # 平均預測（提升穩定性）\n",
        "    'avg_lower': (xgb_lower_oof + lgb_lower_oof + ridge_lower_oof) / 3,\n",
        "    'avg_upper': (xgb_upper_oof + lgb_upper_oof + ridge_upper_oof) / 3,\n",
        "\n",
        "    # 區間寬度（模型可能學會自動拉寬）\n",
        "    'interval_width_xgb': xgb_upper_oof - xgb_lower_oof,\n",
        "    'interval_width_lgb': lgb_upper_oof - lgb_lower_oof,\n",
        "    'interval_width_ridge': ridge_upper_oof - ridge_lower_oof,\n",
        "\n",
        "    # 極端判斷（判斷是否有反轉）\n",
        "    'lower_gt_upper_lgb': (lgb_lower_oof > lgb_upper_oof).astype(int)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006058 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 525018.728175\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007067 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 643911.778866\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003591 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 524608.914022\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003182 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 643410.894906\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003764 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 524653.924262\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006366 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 643465.907412\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005440 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 525252.281522\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003171 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 644197.232952\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003310 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 524138.878151\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006959 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 642836.406635\n",
            "[Step 0] ✅ Improved Score: 1323304.56 (w1: 0.4100, w2: 0.6100)\n",
            "[Step 1] ✅ Improved Score: 1302388.58 (w1: 0.4200, w2: 0.6200)\n",
            "[Step 2] ✅ Improved Score: 1281779.45 (w1: 0.4300, w2: 0.6300)\n",
            "[Step 3] ✅ Improved Score: 1261484.36 (w1: 0.4400, w2: 0.6400)\n",
            "[Step 4] ✅ Improved Score: 1241494.00 (w1: 0.4500, w2: 0.6500)\n",
            "[Step 5] ✅ Improved Score: 1221807.13 (w1: 0.4600, w2: 0.6600)\n",
            "[Step 6] ✅ Improved Score: 1202423.72 (w1: 0.4700, w2: 0.6700)\n",
            "[Step 7] ✅ Improved Score: 1183353.56 (w1: 0.4800, w2: 0.6800)\n",
            "[Step 8] ✅ Improved Score: 1164591.83 (w1: 0.4900, w2: 0.6900)\n",
            "[Step 9] ✅ Improved Score: 1146135.73 (w1: 0.5000, w2: 0.7000)\n",
            "[Step 10] ✅ Improved Score: 1127982.44 (w1: 0.5100, w2: 0.7100)\n",
            "[Step 11] ✅ Improved Score: 1110137.39 (w1: 0.5200, w2: 0.7200)\n",
            "[Step 12] ✅ Improved Score: 1092593.63 (w1: 0.5300, w2: 0.7300)\n",
            "[Step 13] ✅ Improved Score: 1075347.53 (w1: 0.5400, w2: 0.7400)\n",
            "[Step 14] ✅ Improved Score: 1058390.44 (w1: 0.5500, w2: 0.7500)\n",
            "[Step 15] ✅ Improved Score: 1041725.52 (w1: 0.5600, w2: 0.7600)\n",
            "[Step 16] ✅ Improved Score: 1025360.04 (w1: 0.5700, w2: 0.7700)\n",
            "[Step 17] ✅ Improved Score: 1009286.10 (w1: 0.5800, w2: 0.7800)\n",
            "[Step 18] ✅ Improved Score: 993501.75 (w1: 0.5900, w2: 0.7900)\n",
            "[Step 19] ✅ Improved Score: 978010.50 (w1: 0.6000, w2: 0.8000)\n",
            "[Step 20] ✅ Improved Score: 962813.73 (w1: 0.6100, w2: 0.8100)\n",
            "[Step 21] ✅ Improved Score: 947912.11 (w1: 0.6200, w2: 0.8200)\n",
            "[Step 22] ✅ Improved Score: 933287.00 (w1: 0.6300, w2: 0.8300)\n",
            "[Step 23] ✅ Improved Score: 918945.36 (w1: 0.6400, w2: 0.8400)\n",
            "[Step 24] ✅ Improved Score: 904885.08 (w1: 0.6500, w2: 0.8500)\n",
            "[Step 25] ✅ Improved Score: 891092.50 (w1: 0.6600, w2: 0.8600)\n",
            "[Step 26] ✅ Improved Score: 877578.03 (w1: 0.6700, w2: 0.8700)\n",
            "[Step 27] ✅ Improved Score: 864342.03 (w1: 0.6800, w2: 0.8800)\n",
            "[Step 28] ✅ Improved Score: 851375.72 (w1: 0.6900, w2: 0.8900)\n",
            "[Step 29] ✅ Improved Score: 838682.66 (w1: 0.7000, w2: 0.9000)\n",
            "[Step 30] ✅ Improved Score: 826259.29 (w1: 0.7100, w2: 0.9100)\n",
            "[Step 31] ✅ Improved Score: 814102.31 (w1: 0.7200, w2: 0.9200)\n",
            "[Step 32] ✅ Improved Score: 802203.71 (w1: 0.7300, w2: 0.9300)\n",
            "[Step 33] ✅ Improved Score: 790572.07 (w1: 0.7400, w2: 0.9400)\n",
            "[Step 34] ✅ Improved Score: 779193.92 (w1: 0.7500, w2: 0.9500)\n",
            "[Step 35] ✅ Improved Score: 768063.88 (w1: 0.7600, w2: 0.9600)\n",
            "[Step 36] ✅ Improved Score: 757180.54 (w1: 0.7700, w2: 0.9700)\n",
            "[Step 37] ✅ Improved Score: 746542.95 (w1: 0.7800, w2: 0.9800)\n",
            "[Step 38] ✅ Improved Score: 736155.22 (w1: 0.7900, w2: 0.9900)\n",
            "[Step 39] ✅ Improved Score: 726013.09 (w1: 0.8000, w2: 1.0000)\n",
            "[Step 40] ✅ Improved Score: 719601.90 (w1: 0.8100, w2: 1.0000)\n",
            "[Step 41] ✅ Improved Score: 713338.21 (w1: 0.8200, w2: 1.0000)\n",
            "[Step 42] ✅ Improved Score: 707212.19 (w1: 0.8300, w2: 1.0000)\n",
            "[Step 43] ✅ Improved Score: 701233.06 (w1: 0.8400, w2: 1.0000)\n",
            "[Step 44] ✅ Improved Score: 695405.13 (w1: 0.8500, w2: 1.0000)\n",
            "[Step 45] ✅ Improved Score: 689723.05 (w1: 0.8600, w2: 1.0000)\n",
            "[Step 46] ✅ Improved Score: 684176.32 (w1: 0.8700, w2: 1.0000)\n",
            "[Step 47] ✅ Improved Score: 678761.51 (w1: 0.8800, w2: 1.0000)\n",
            "[Step 48] ✅ Improved Score: 673476.43 (w1: 0.8900, w2: 1.0000)\n",
            "[Step 49] ✅ Improved Score: 668326.40 (w1: 0.9000, w2: 1.0000)\n",
            "[Step 50] ✅ Improved Score: 663305.77 (w1: 0.9100, w2: 1.0000)\n",
            "[Step 51] ✅ Improved Score: 658414.37 (w1: 0.9200, w2: 1.0000)\n",
            "[Step 52] ✅ Improved Score: 653648.03 (w1: 0.9300, w2: 1.0000)\n",
            "[Step 53] ✅ Improved Score: 649001.62 (w1: 0.9400, w2: 1.0000)\n",
            "[Step 54] ✅ Improved Score: 644473.39 (w1: 0.9500, w2: 1.0000)\n",
            "[Step 55] ✅ Improved Score: 640062.74 (w1: 0.9600, w2: 1.0000)\n",
            "[Step 56] ✅ Improved Score: 635772.24 (w1: 0.9700, w2: 1.0000)\n",
            "[Step 57] ✅ Improved Score: 631604.80 (w1: 0.9800, w2: 1.0000)\n",
            "[Step 58] ✅ Improved Score: 627558.45 (w1: 0.9900, w2: 1.0000)\n",
            "[Step 59] ✅ Improved Score: 623629.41 (w1: 1.0000, w2: 1.0000)\n"
          ]
        }
      ],
      "source": [
        "oof_lowers, oof_uppers, best_weight, best_score = oof_and_hill_climb_with_meta(X_meta, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "before plus floor ratio~bath per bed score = 341636.83\n",
        "after plus new feature score = 342013\n",
        "after adjust model = 387816"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "輸出test LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#填補缺漏欄位（對齊訓練集欄位）\n",
        "missing_cols = set(X.columns) - set(test_encoded.columns)\n",
        "for col in missing_cols:\n",
        "    test_encoded[col] = 0\n",
        "\n",
        "# 確保欄位順序一致\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.005452 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004503 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1435000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004989 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004292 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004324 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003735 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1431000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004233 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003492 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1440000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004044 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003498 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[Step 0] Improved MWIS: 6132346.42 (Weight: 0.4100)\n",
            "[Step 1] Improved MWIS: 5998138.12 (Weight: 0.4200)\n",
            "[Step 2] Improved MWIS: 5864083.19 (Weight: 0.4300)\n",
            "[Step 3] Improved MWIS: 5730199.30 (Weight: 0.4400)\n",
            "[Step 4] Improved MWIS: 5596511.15 (Weight: 0.4500)\n",
            "[Step 5] Improved MWIS: 5463018.77 (Weight: 0.4600)\n",
            "[Step 6] Improved MWIS: 5329716.49 (Weight: 0.4700)\n",
            "[Step 7] Improved MWIS: 5196602.32 (Weight: 0.4800)\n",
            "[Step 8] Improved MWIS: 5063747.28 (Weight: 0.4900)\n",
            "[Step 9] Improved MWIS: 4931168.66 (Weight: 0.5000)\n",
            "[Step 10] Improved MWIS: 4798872.28 (Weight: 0.5100)\n",
            "[Step 11] Improved MWIS: 4666890.87 (Weight: 0.5200)\n",
            "[Step 12] Improved MWIS: 4535241.47 (Weight: 0.5300)\n",
            "[Step 13] Improved MWIS: 4403949.01 (Weight: 0.5400)\n",
            "[Step 14] Improved MWIS: 4273021.68 (Weight: 0.5500)\n",
            "[Step 15] Improved MWIS: 4142466.76 (Weight: 0.5600)\n",
            "[Step 16] Improved MWIS: 4012315.06 (Weight: 0.5700)\n",
            "[Step 17] Improved MWIS: 3882623.63 (Weight: 0.5800)\n",
            "[Step 18] Improved MWIS: 3753424.52 (Weight: 0.5900)\n",
            "[Step 19] Improved MWIS: 3624740.01 (Weight: 0.6000)\n",
            "[Step 20] Improved MWIS: 3496604.74 (Weight: 0.6100)\n",
            "[Step 21] Improved MWIS: 3369087.47 (Weight: 0.6200)\n",
            "[Step 22] Improved MWIS: 3242267.39 (Weight: 0.6300)\n",
            "[Step 23] Improved MWIS: 3116209.35 (Weight: 0.6400)\n",
            "[Step 24] Improved MWIS: 2991022.61 (Weight: 0.6500)\n",
            "[Step 25] Improved MWIS: 2866738.68 (Weight: 0.6600)\n",
            "[Step 26] Improved MWIS: 2743466.81 (Weight: 0.6700)\n",
            "[Step 27] Improved MWIS: 2621243.19 (Weight: 0.6800)\n",
            "[Step 28] Improved MWIS: 2500209.55 (Weight: 0.6900)\n",
            "[Step 29] Improved MWIS: 2380487.12 (Weight: 0.7000)\n",
            "[Step 30] Improved MWIS: 2262178.94 (Weight: 0.7100)\n",
            "[Step 31] Improved MWIS: 2145526.51 (Weight: 0.7200)\n",
            "[Step 32] Improved MWIS: 2030689.68 (Weight: 0.7300)\n",
            "[Step 33] Improved MWIS: 1917840.75 (Weight: 0.7400)\n",
            "[Step 34] Improved MWIS: 1807139.16 (Weight: 0.7500)\n",
            "[Step 35] Improved MWIS: 1698786.24 (Weight: 0.7600)\n",
            "[Step 36] Improved MWIS: 1593036.82 (Weight: 0.7700)\n",
            "[Step 37] Improved MWIS: 1490133.56 (Weight: 0.7800)\n",
            "[Step 38] Improved MWIS: 1390254.94 (Weight: 0.7900)\n",
            "[Step 39] Improved MWIS: 1293675.49 (Weight: 0.8000)\n",
            "[Step 40] Improved MWIS: 1200790.81 (Weight: 0.8100)\n",
            "[Step 41] Improved MWIS: 1111796.51 (Weight: 0.8200)\n",
            "[Step 42] Improved MWIS: 1026969.04 (Weight: 0.8300)\n",
            "[Step 43] Improved MWIS: 946610.10 (Weight: 0.8400)\n",
            "[Step 44] Improved MWIS: 870883.14 (Weight: 0.8500)\n",
            "[Step 45] Improved MWIS: 800085.84 (Weight: 0.8600)\n",
            "[Step 46] Improved MWIS: 734358.53 (Weight: 0.8700)\n",
            "[Step 47] Improved MWIS: 673869.19 (Weight: 0.8800)\n",
            "[Step 48] Improved MWIS: 618718.30 (Weight: 0.8900)\n",
            "[Step 49] Improved MWIS: 569060.83 (Weight: 0.9000)\n",
            "[Step 50] Improved MWIS: 524864.35 (Weight: 0.9100)\n",
            "[Step 51] Improved MWIS: 486048.75 (Weight: 0.9200)\n",
            "[Step 52] Improved MWIS: 452383.38 (Weight: 0.9300)\n",
            "[Step 53] Improved MWIS: 423677.89 (Weight: 0.9400)\n",
            "[Step 54] Improved MWIS: 399765.80 (Weight: 0.9500)\n",
            "[Step 55] Improved MWIS: 380256.93 (Weight: 0.9600)\n",
            "[Step 56] Improved MWIS: 364930.36 (Weight: 0.9700)\n",
            "[Step 57] Improved MWIS: 353482.91 (Weight: 0.9800)\n",
            "[Step 58] Improved MWIS: 345674.64 (Weight: 0.9900)\n",
            "[Step 59] Improved MWIS: 341379.28 (Weight: 1.0000)\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3856\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (5.34 MB) transferred to GPU in 0.005645 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3856\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (5.34 MB) transferred to GPU in 0.004793 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1434006.000000\n"
          ]
        }
      ],
      "source": [
        "model_lower = models[\"lower\"]\n",
        "model_upper = models[\"upper\"]\n",
        "\n",
        "final_model_lower = clone(model_lower).fit(X, y)\n",
        "final_model_upper = clone(model_upper).fit(X, y)\n",
        "\n",
        "'''test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = best_weight * test_lower\n",
        "final_upper = best_weight * test_upper\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)'''\n",
        "\n",
        "\n",
        "test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = w1 * test_lower + (1 - w1) * test_upper\n",
        "final_upper = w2 * test_upper + (1 - w2) * test_lower\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)  # optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "submission_df.head()\n",
        "test_encoded['id'] = test_df['id']  # 這行先補上 id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       id       pi_lower      pi_upper\n",
            "0  200000  823500.777056  1.125383e+06\n",
            "1  200001  590158.900174  7.270068e+05\n",
            "2  200002  469229.953050  6.502398e+05\n",
            "3  200003  315274.697956  4.363184e+05\n",
            "4  200004  413919.532224  6.498657e+05\n"
          ]
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'id': test_encoded['id'],  # 必須與 sample_submission 對齊\n",
        "    'pi_lower': final_lower,\n",
        "    'pi_upper': final_upper\n",
        "})\n",
        "\n",
        "# 輸出成 CSV\n",
        "submission_df.to_csv('lgbm_predict.csv', index=False)\n",
        "print(submission_df.head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_tensor_evn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
