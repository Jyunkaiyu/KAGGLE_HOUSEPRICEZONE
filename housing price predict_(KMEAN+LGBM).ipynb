{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "train_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\dataset.csv\"\n",
        "test_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\test.csv\"\n",
        "sam_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\sample_submission.csv\"\n",
        "\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000, 47)\n",
            "Index(['id', 'sale_date', 'sale_price', 'sale_nbr', 'sale_warning',\n",
            "       'join_status', 'join_year', 'latitude', 'longitude', 'area', 'city',\n",
            "       'zoning', 'subdivision', 'present_use', 'land_val', 'imp_val',\n",
            "       'year_built', 'year_reno', 'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
            "       'grade', 'fbsmt_grade', 'condition', 'stories', 'beds', 'bath_full',\n",
            "       'bath_3qtr', 'bath_half', 'garb_sqft', 'gara_sqft', 'wfnt', 'golf',\n",
            "       'greenbelt', 'noise_traffic', 'view_rainier', 'view_olympics',\n",
            "       'view_cascades', 'view_territorial', 'view_skyline', 'view_sound',\n",
            "       'view_lakewash', 'view_lakesamm', 'view_otherwater', 'view_other',\n",
            "       'submarket'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sale_date</th>\n",
              "      <th>sale_price</th>\n",
              "      <th>sale_nbr</th>\n",
              "      <th>sale_warning</th>\n",
              "      <th>join_status</th>\n",
              "      <th>join_year</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>area</th>\n",
              "      <th>...</th>\n",
              "      <th>view_olympics</th>\n",
              "      <th>view_cascades</th>\n",
              "      <th>view_territorial</th>\n",
              "      <th>view_skyline</th>\n",
              "      <th>view_sound</th>\n",
              "      <th>view_lakewash</th>\n",
              "      <th>view_lakesamm</th>\n",
              "      <th>view_otherwater</th>\n",
              "      <th>view_other</th>\n",
              "      <th>submarket</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-15</td>\n",
              "      <td>236000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.2917</td>\n",
              "      <td>-122.3658</td>\n",
              "      <td>53</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1999-01-15</td>\n",
              "      <td>313300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.6531</td>\n",
              "      <td>-122.1996</td>\n",
              "      <td>74</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2006-08-15</td>\n",
              "      <td>341000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4733</td>\n",
              "      <td>-122.1901</td>\n",
              "      <td>30</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1999-12-15</td>\n",
              "      <td>267000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4739</td>\n",
              "      <td>-122.3295</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-15</td>\n",
              "      <td>1650000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>miss99</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.7516</td>\n",
              "      <td>-122.1222</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 47 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id   sale_date  sale_price  sale_nbr sale_warning join_status  join_year  \\\n",
              "0   0  2014-11-15      236000       2.0                    nochg       2025   \n",
              "1   1  1999-01-15      313300       NaN          26        nochg       2025   \n",
              "2   2  2006-08-15      341000       1.0                    nochg       2025   \n",
              "3   3  1999-12-15      267000       1.0                    nochg       2025   \n",
              "4   4  2018-07-15     1650000       2.0                   miss99       2025   \n",
              "\n",
              "   latitude  longitude  area  ... view_olympics view_cascades  \\\n",
              "0   47.2917  -122.3658    53  ...             0             0   \n",
              "1   47.6531  -122.1996    74  ...             0             0   \n",
              "2   47.4733  -122.1901    30  ...             0             0   \n",
              "3   47.4739  -122.3295    96  ...             0             0   \n",
              "4   47.7516  -122.1222    36  ...             0             0   \n",
              "\n",
              "  view_territorial  view_skyline  view_sound  view_lakewash  view_lakesamm  \\\n",
              "0                0             0           0              0              0   \n",
              "1                0             0           0              1              0   \n",
              "2                0             0           0              0              0   \n",
              "3                0             0           0              0              0   \n",
              "4                0             0           0              0              0   \n",
              "\n",
              "   view_otherwater  view_other  submarket  \n",
              "0                0           0          I  \n",
              "1                0           0          Q  \n",
              "2                0           0          K  \n",
              "3                0           0          G  \n",
              "4                0           0          P  \n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "execution_count": 240,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "sample_df = pd.read_csv(sam_path)\n",
        "\n",
        "print(train_df.shape)\n",
        "print(train_df.columns)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sale_nbr       42182\n",
              "subdivision    17550\n",
              "submarket       1717\n",
              "dtype: int64"
            ]
          },
          "execution_count": 241,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#計算缺失值\n",
        "missing_values = train_df.isnull().sum()\n",
        "missing_values[missing_values > 0].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 train_encoded\n",
        "processed_cols = []\n",
        "train_encoded = pd.DataFrame()\n",
        "\n",
        "# One-hot \n",
        "confirmed_one_hot = [\n",
        "    'join_status', 'condition', 'stories', 'grade', 'fbsmt_grade', 'present_use'\n",
        "]\n",
        "onehot_df = pd.get_dummies(train_df[confirmed_one_hot], drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, onehot_df], axis=1)\n",
        "processed_cols += confirmed_one_hot\n",
        "\n",
        "\n",
        "train_df['sale_date'] = pd.to_datetime(train_df['sale_date'], errors='coerce')\n",
        "train_encoded['sale_year'] = train_df['sale_date'].dt.year\n",
        "train_encoded['sale_month'] = train_df['sale_date'].dt.month\n",
        "train_encoded['sale_season'] = ((train_encoded['sale_month'] - 1) // 3 + 1)\n",
        "processed_cols += ['sale_date']\n",
        "\n",
        "# 原始數值直接加入\n",
        "direct_add_cols = [\n",
        "    'id', 'sale_price', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half', 'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'  to much missing value\n",
        "]\n",
        "for col in direct_add_cols:\n",
        "    train_encoded[col] = train_df[col]\n",
        "processed_cols += direct_add_cols\n",
        "\n",
        "# 統整城市、市場與銷售警告資訊\n",
        "top_cities = train_df['city'].value_counts().nlargest(10).index.tolist()\n",
        "top_supermarket = train_df['submarket'].value_counts().nlargest(10).index.tolist()\n",
        "top_sale_warning = train_df['sale_warning'].value_counts().nlargest(15).index.tolist()\n",
        "\n",
        "train_encoded['city_simplified'] = train_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "train_encoded['submarket_simplified'] = train_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "train_encoded['sale_warning_simplified'] = train_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "\n",
        "city_dummy = pd.get_dummies(train_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "submarket_dummy = pd.get_dummies(train_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "sale_warning_dummy = pd.get_dummies(train_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, city_dummy, submarket_dummy, sale_warning_dummy], axis=1)\n",
        "#train_encoded = pd.concat([train_encoded, city_dummy], axis=1)\n",
        "processed_cols += ['city', 'submarket', 'sale_warning']\n",
        "\n",
        "# Zoning 群組分類\n",
        "def zoning_group_classify(z):\n",
        "    if pd.isna(z): return 'other'\n",
        "    z = z.upper()\n",
        "    if 'SF' in z: return 'SF'\n",
        "    elif 'MR' in z: return 'MR'\n",
        "    elif 'NC' in z: return 'NC'\n",
        "    elif 'HR' in z or 'IG' in z: return 'other'\n",
        "    elif 'P' in z: return 'P'\n",
        "    return 'other'\n",
        "\n",
        "train_encoded['zoning_group'] = train_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(train_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, zoning_dummy], axis=1)\n",
        "train_encoded.drop(columns=['zoning_group'], inplace=True)\n",
        "processed_cols += ['zoning']\n",
        "\n",
        "\n",
        "# 碎片化資訊統整成新欄位\n",
        "train_encoded['age'] = train_encoded['sale_year'] - train_encoded['year_built']\n",
        "train_encoded['renovated'] = np.where(train_encoded['year_reno'] > 0, 1, 0)\n",
        "train_encoded['years_since_reno'] = np.where(train_encoded['renovated'], train_encoded['sale_year'] - train_encoded['year_reno'], 0)\n",
        "train_encoded['total_baths'] = train_encoded['bath_full'] + 0.75 * train_encoded['bath_3qtr'] + 0.5 * train_encoded['bath_half']\n",
        "train_encoded['total_value'] = train_encoded['land_val'] + train_encoded['imp_val']\n",
        "train_encoded['living_area'] = train_encoded['sqft'] + train_encoded['sqft_fbsmt']\n",
        "\n",
        "# 刪除用完的簡化文字類欄位\n",
        "for col in ['city_simplified', 'submarket_simplified', 'sale_warning_simplified']:\n",
        "    train_encoded.drop(columns=[col], inplace=True)\n",
        "\n",
        "#for col in ['city_simplified']:\n",
        "#    train_encoded.drop(columns=[col], inplace=True)\n",
        "#新增特徵\n",
        "non_zero_lot = train_encoded.loc[train_encoded[\"sqft_lot\"] > 0, \"sqft_lot\"]\n",
        "min_val = non_zero_lot.min()\n",
        "median_val = non_zero_lot.median()\n",
        "\n",
        "train_encoded[\"sqft_lot\"] = train_encoded[\"sqft_lot\"].replace(0, median_val)\n",
        "\n",
        "#新增特徵\n",
        "train_encoded[\"floor_ratio\"] = np.where(\n",
        "    train_encoded[\"sqft_lot\"] == 0,\n",
        "    0,\n",
        "    train_encoded[\"sqft\"] / train_encoded[\"sqft_lot\"]\n",
        ")\n",
        "\n",
        "train_encoded[\"is_large_house\"] = (train_encoded[\"sqft\"] > 3000).astype(int)\n",
        "train_encoded[\"is_recent_reno\"] = (train_encoded[\"years_since_reno\"] <= 5).astype(int)\n",
        "train_encoded[\"bath_per_bed\"] = train_encoded[\"total_baths\"] / train_encoded[\"beds\"]\n",
        "train_encoded[\"bath_per_bed\"] = train_encoded[\"bath_per_bed\"].replace([np.inf, -np.inf], 0).fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "pca_features = ['latitude', 'longitude', 'sqft', 'area', 'total_value', 'imp_val']\n",
        "\n",
        "# 🔃 標準化 → PCA → KMeans\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(train_encoded[pca_features])\n",
        "\n",
        "pca = PCA(n_components=3, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "train_encoded['pca_region_cluster'] = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 🔄 One-hot 編碼\n",
        "region_dummies = pd.get_dummies(train_encoded['pca_region_cluster'], prefix='pca_region')\n",
        "train_encoded = pd.concat([train_encoded, region_dummies], axis=1)\n",
        "\n",
        "train_encoded.drop(columns=['pca_region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_features(df):\n",
        "    import numpy as np\n",
        "\n",
        "    # 建議 log1p 處理（避免極端偏態影響模型）\n",
        "    log_cols = ['land_val', 'imp_val', 'sqft_lot', 'garb_sqft', 'floor_ratio', 'total_value']\n",
        "    #log_cols = ['land_val', 'imp_val', 'sqft_lot', 'garb_sqft', 'total_value']\n",
        "    for col in log_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.log1p(df[col])\n",
        "\n",
        "    # clip 上限值（可選，如果你不 log）\n",
        "    clip_cols = ['land_val', 'imp_val', 'sqft_lot']\n",
        "    for col in clip_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].clip(upper=1_000_000)\n",
        "            \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_encoded = clean_features(train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [],
      "source": [
        "direct_add_cols = [\n",
        "    'id', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half',\n",
        "    'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'沒有做這個 用意不大\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 test_encoded 空表\n",
        "test_encoded = pd.DataFrame()\n",
        "\n",
        "# 1. One-hot 欄位\n",
        "test_onehot_df = pd.get_dummies(test_df[confirmed_one_hot], drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, test_onehot_df], axis=1)\n",
        "\n",
        "# 2. 日期處理\n",
        "test_df['sale_date'] = pd.to_datetime(test_df['sale_date'], errors='coerce')\n",
        "test_encoded['sale_year'] = test_df['sale_date'].dt.year\n",
        "test_encoded['sale_month'] = test_df['sale_date'].dt.month\n",
        "test_encoded['sale_season'] = ((test_encoded['sale_month'] - 1) // 3 + 1)\n",
        "\n",
        "# 3. 加入 direct_add_cols 欄位\n",
        "for col in direct_add_cols:\n",
        "    test_encoded[col] = test_df[col]\n",
        "\n",
        "# 4. city / submarket / sale_warning (simplified)\n",
        "test_encoded['city_simplified'] = test_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "city_dummy = pd.get_dummies(test_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, city_dummy], axis=1)\n",
        "\n",
        "test_encoded['submarket_simplified'] = test_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "submarket_dummy = pd.get_dummies(test_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, submarket_dummy], axis=1)\n",
        "\n",
        "test_encoded['sale_warning_simplified'] = test_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "sale_warning_dummy = pd.get_dummies(test_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, sale_warning_dummy], axis=1)\n",
        "\n",
        "# 5. Zoning 分群 One-hot\n",
        "test_encoded['zoning_group'] = test_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(test_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, zoning_dummy], axis=1)\n",
        "test_encoded.drop(columns=['zoning_group', 'city_simplified', 'submarket_simplified', 'sale_warning_simplified'], inplace=True)\n",
        "#test_encoded.drop(columns=['zoning_group', 'city_simplified'], inplace=True)\n",
        "\n",
        "#碎片化資訊統整成新欄位\n",
        "test_encoded['age'] = test_encoded['sale_year'] - test_encoded['year_built']\n",
        "test_encoded['renovated'] = np.where(test_encoded['year_reno'] > 0, 1, 0)\n",
        "test_encoded['years_since_reno'] = np.where(test_encoded['renovated'], test_encoded['sale_year'] - test_encoded['year_reno'], 0)\n",
        "test_encoded['total_baths'] = test_encoded['bath_full'] + 0.75 * test_encoded['bath_3qtr'] + 0.5 * test_encoded['bath_half']\n",
        "test_encoded['total_value'] = test_encoded['land_val'] + test_encoded['imp_val']\n",
        "test_encoded['living_area'] = test_encoded['sqft'] + test_encoded['sqft_fbsmt']\n",
        "\n",
        "non_zero_lot = test_encoded.loc[test_encoded[\"sqft_lot\"] > 0, \"sqft_lot\"]\n",
        "min_val = non_zero_lot.min()\n",
        "median_val = non_zero_lot.median()\n",
        "\n",
        "test_encoded[\"sqft_lot\"] = test_encoded[\"sqft_lot\"].replace(0, median_val)\n",
        "\n",
        "\n",
        "test_encoded[\"floor_ratio\"] = np.where(\n",
        "    test_encoded[\"sqft_lot\"] == 0,\n",
        "    0,  # 或其他替代值，例如平均值\n",
        "    test_encoded[\"sqft\"] / test_encoded[\"sqft_lot\"]\n",
        ")\n",
        "\n",
        "test_encoded[\"is_large_house\"] = (test_encoded[\"sqft\"] > 3000).astype(int)\n",
        "test_encoded[\"is_recent_reno\"] = (test_encoded[\"years_since_reno\"] <= 5).astype(int)\n",
        "test_encoded[\"bath_per_bed\"] = test_encoded[\"total_baths\"] / test_encoded[\"beds\"]\n",
        "test_encoded[\"bath_per_bed\"] = test_encoded[\"bath_per_bed\"].replace([np.inf, -np.inf], 0).fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\anaconda3\\envs\\ai_tensor_evn\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "pca_features = ['latitude', 'longitude', 'sqft', 'area', 'total_value', 'imp_val']\n",
        "\n",
        "# 🔃 標準化 → PCA → KMeans\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(test_encoded[pca_features])\n",
        "\n",
        "pca = PCA(n_components=3, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "test_encoded['pca_region_cluster'] = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# 🔄 One-hot 編碼\n",
        "region_dummies = pd.get_dummies(test_encoded['pca_region_cluster'], prefix='pca_region')\n",
        "test_encoded = pd.concat([test_encoded, region_dummies], axis=1)\n",
        "\n",
        "test_encoded.drop(columns=['pca_region_cluster'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_encoded = clean_features(test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "del_train = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "\n",
        "# 計算每個欄位與 y 的皮爾森相關係數\n",
        "correlations = del_train.corrwith(train_encoded['sale_price']).abs().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def reduce_to_top_features(df, top_features, target_col='sale_price', id_col='id'):\n",
        "    \"\"\"\n",
        "    根據 top_features 保留資料中的前幾重要特徵，加上 id 及 target 欄位（若存在）供後續使用\n",
        "    \"\"\"\n",
        "    cols_to_keep = [col for col in [id_col, target_col] if col in df.columns] + top_features\n",
        "    return df[cols_to_keep].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 先取得前 60 個特徵名稱\n",
        "top_60_features = correlations.head(60).index.tolist()\n",
        "\n",
        "train_encoded = reduce_to_top_features(train_encoded, top_60_features)\n",
        "test_encoded = reduce_to_top_features(test_encoded, top_60_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bool       27\n",
            "int64      22\n",
            "float64     9\n",
            "int32       4\n",
            "Name: count, dtype: int64\n",
            "bool       27\n",
            "int64      21\n",
            "float64     9\n",
            "int32       4\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#確認資料類型\n",
        "print(train_encoded.dtypes.value_counts())\n",
        "print(test_encoded.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set()\n"
          ]
        }
      ],
      "source": [
        "# train/test 欄位補齊\n",
        "missing_cols = set(train_encoded.columns) - set(test_encoded.columns) - {\"sale_price\"}\n",
        "'''for col in missing_cols:\n",
        "    test_encoded[col] = 0'''\n",
        "print(missing_cols)\n",
        "# ✅ 欄位順序一致化\n",
        "test_encoded = test_encoded[train_encoded.drop(columns=[\"sale_price\"]).columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model(XGBoost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "def winkler_score(y_true, y_lower, y_upper, alpha=0.1):\n",
        "    interval_width = y_upper - y_lower\n",
        "    is_covered = (y_true >= y_lower) & (y_true <= y_upper)\n",
        "    penalty = 2 / alpha * ((y_lower - y_true).clip(lower=0) + (y_true - y_upper).clip(lower=0))\n",
        "    score = interval_width + penalty\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def oof_and_hill_climb_xgb(X, y, model_lower, model_upper, alpha=0.1, n_splits=5, steps=100, seed=42):\n",
        "    oof_lowers = np.zeros(len(X))\n",
        "    oof_uppers = np.zeros(len(X))\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "        y_lower_train = y_train * (1 - alpha)\n",
        "        y_upper_train = y_train * (1 + alpha)\n",
        "        y_val_true = y.iloc[val_idx]\n",
        "        y_lower_val = y_val_true * (1 - alpha)\n",
        "        y_upper_val = y_val_true * (1 + alpha)\n",
        "\n",
        "        # XGBoost clone\n",
        "        lower = clone(model_lower)\n",
        "        upper = clone(model_upper)\n",
        "\n",
        "        lower.fit(X_train, y_lower_train,\n",
        "                  eval_set=[(X_val, y_lower_val)],\n",
        "                  verbose=False)\n",
        "        upper.fit(X_train, y_upper_train,\n",
        "                  eval_set=[(X_val, y_upper_val)],\n",
        "                  verbose=False)\n",
        "\n",
        "        oof_lowers[val_idx] = lower.predict(X_val)\n",
        "        oof_uppers[val_idx] = upper.predict(X_val)\n",
        "\n",
        "    # 開始雙權重尋找（Hill Climb）\n",
        "    best_score = np.inf\n",
        "    best_weights = (0.4, 0.6)\n",
        "\n",
        "    for w1 in np.linspace(0.0, 1.0, 25):\n",
        "        for w2 in np.linspace(0.0, 1.0, 25):\n",
        "            lower_comb = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "            upper_comb = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "            lower_comb, upper_comb = np.minimum(lower_comb, upper_comb), np.maximum(lower_comb, upper_comb)\n",
        "            score = np.mean(winkler_score(y, lower_comb, upper_comb, alpha))\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_weights = (w1, w2)\n",
        "\n",
        "    print(f\"✅ Best Winkler score: {best_score:.2f}, weights: w1={best_weights[0]:.4f}, w2={best_weights[1]:.4f}\")\n",
        "    return oof_lowers, oof_uppers, best_weights, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lower = xgb.XGBRegressor(\n",
        "    n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "    early_stopping_rounds=20, eval_metric='rmse', tree_method='gpu_hist', random_state=42\n",
        ")\n",
        "\n",
        "model_upper = xgb.XGBRegressor(\n",
        "    n_estimators=500, max_depth=6, learning_rate=0.01,\n",
        "    early_stopping_rounds=20, eval_metric='rmse', tree_method='gpu_hist', random_state=42\n",
        ")\n",
        "\n",
        "oof_lowers, oof_uppers, best_weight, best_score = oof_and_hill_climb_xgb(\n",
        "    X, y, model_lower, model_upper, alpha=0.1, n_splits=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LGBM Quantile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
        "    width = upper - lower\n",
        "    below = np.maximum(lower - y_true, 0)\n",
        "    above = np.maximum(y_true - upper, 0)\n",
        "    return width + (2 / alpha) * (below + above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {},
      "outputs": [],
      "source": [
        "def oof_and_hill_climb_two_weights(X, y, model_lower, model_upper, alpha=0.1, n_splits=5, seed=42, steps=100 , grid_steps=25):\n",
        "    oof_lowers = np.zeros(len(X))\n",
        "    oof_uppers = np.zeros(len(X))\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        lower_model = clone(model_lower)\n",
        "        upper_model = clone(model_upper)\n",
        "\n",
        "        lower_model.fit(X_train, y_train)\n",
        "        upper_model.fit(X_train, y_train)\n",
        "\n",
        "        oof_lowers[val_idx] = lower_model.predict(X_val)\n",
        "        oof_uppers[val_idx] = upper_model.predict(X_val)\n",
        "\n",
        "    '''# 初始化雙權重\n",
        "    current_w1 = 0.4  # 下限 weight\n",
        "    current_w2 = 0.6  # 上限 weight\n",
        "\n",
        "    best_score = np.inf\n",
        "    best_weights = (current_w1, current_w2)'''\n",
        "\n",
        "    best_score = np.inf\n",
        "    best_weights = (0.4, 0.6)  # 預設初始點\n",
        "    grid_range = np.linspace(0.0, 1.0, grid_steps)\n",
        "\n",
        "    for w1 in grid_range:\n",
        "        for w2 in grid_range:\n",
        "            lower_comb = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "            upper_comb = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "            lower_comb, upper_comb = np.minimum(lower_comb, upper_comb), np.maximum(lower_comb, upper_comb)\n",
        "            score = np.mean(winkler_score(y, lower_comb, upper_comb, alpha))\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_weights = (w1, w2)\n",
        "\n",
        "    current_w1, current_w2 = best_weights\n",
        "    no_improve_count = 0\n",
        "\n",
        "    for step in range(steps):\n",
        "        '''# 微調 perturbation\n",
        "        dw1, dw2 = np.random.uniform(-0.1, 0.1, size=2)\n",
        "        w1 = np.clip(current_w1 + dw1, 0.1, 0.9)  # 限制在合理區間\n",
        "        w2 = np.clip(current_w2 + dw2, 0.1, 0.9)'''\n",
        "\n",
        "        # 微調 perturbation，讓 weight 有隨機性（避免卡住）\n",
        "        perturb1 = np.random.dirichlet([9])[0] - 0.9\n",
        "        perturb2 = np.random.dirichlet([9])[0] - 0.9\n",
        "\n",
        "        w1 = np.clip(current_w1 + 0.1 * perturb1, 0, 1)\n",
        "        w2 = np.clip(current_w2 + 0.1 * perturb2, 0, 1)\n",
        "\n",
        "        # 雙權重組合\n",
        "        lower_combined = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "        upper_combined = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "\n",
        "        # 修正：確保上下限方向正確（防止預測範圍錯位）\n",
        "        lower_combined, upper_combined = np.minimum(lower_combined, upper_combined), np.maximum(lower_combined, upper_combined)\n",
        "\n",
        "        score = np.mean(winkler_score(y, lower_combined, upper_combined, alpha))\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_weights = (w1, w2)\n",
        "            current_w1, current_w2 = w1, w2\n",
        "            print(f\"[Step {step}] ✅ Improved Score: {best_score:.2f} (w1: {w1:.4f}, w2: {w2:.4f})\")\n",
        "        else:\n",
        "            no_improve_count += 1\n",
        "            print(f\"[Step {step}] no improvement. score = {score:.2f} (w1: {w1:.4f}, w2: {w2:.4f})\")\n",
        "\n",
        "        if no_improve_count > 50:  # 超過 50 步沒進步就提早停\n",
        "            break\n",
        "\n",
        "    return oof_lowers, oof_uppers, best_weights, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"lower\": lgb.LGBMRegressor(\n",
        "        objective=\"quantile\",\n",
        "        alpha=0.05,\n",
        "        device=\"cpu\",\n",
        "        n_estimators=1500,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        subsample_freq=1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"upper\": lgb.LGBMRegressor(\n",
        "        objective=\"quantile\",\n",
        "        alpha=0.95,\n",
        "        device=\"cpu\",\n",
        "        n_estimators=1500,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        subsample_freq=1,\n",
        "        random_state=42\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015961 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3213\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016802 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3213\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 1435000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3208\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016149 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3208\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014755 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3203\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013252 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3203\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 1431000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012807 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3203\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013350 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3203\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 1440000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014426 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3206\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011579 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3206\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[Step 0] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 1] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 2] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 3] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 4] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 5] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 6] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 7] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 8] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 9] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 10] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 11] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 12] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 13] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 14] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 15] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 16] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 17] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 18] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 19] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 20] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 21] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 22] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 23] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 24] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 25] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 26] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 27] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 28] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 29] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 30] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 31] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 32] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 33] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 34] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 35] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 36] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 37] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 38] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 39] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 40] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 41] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 42] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 43] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 44] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 45] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 46] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 47] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 48] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 49] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n",
            "[Step 50] no improvement. score = 392732.02 (w1: 0.0100, w2: 0.0100)\n"
          ]
        }
      ],
      "source": [
        "oof_lowers, oof_uppers, (w1, w2), best_score = oof_and_hill_climb_two_weights(\n",
        "    X, y,\n",
        "    model_lower=models[\"lower\"],\n",
        "    model_upper=models[\"upper\"],\n",
        "    alpha=0.1, \n",
        "    n_splits=5,\n",
        "    steps=120\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "before plus floor ratio~bath per bed score = 341590  (original type)\n",
        "after plus new feature score = 342013\n",
        "after adjust model = 387816"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "輸出test xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "#填補缺漏欄位（對齊訓練集欄位）\n",
        "missing_cols = set(X.columns) - set(test_encoded.columns)\n",
        "for col in missing_cols:\n",
        "    test_encoded[col] = 0\n",
        "\n",
        "# 確保欄位順序一致\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set()\n"
          ]
        }
      ],
      "source": [
        "print(set(X.columns) - set(test_encoded.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_encoded['id'] = test_df['id']  # 這行先補上 id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_model_lower = clone(model_lower).fit(X, y * 0.9)\n",
        "final_model_upper = clone(model_upper).fit(X, y * 1.1)\n",
        "\n",
        "test_lower = final_model_lower.predict(test_encoded.drop(columns=['id']))\n",
        "test_upper = final_model_upper.predict(test_encoded.drop(columns=['id']))\n",
        "\n",
        "w1, w2 = best_weight\n",
        "final_lower = w1 * test_lower + (1 - w1) * test_upper\n",
        "final_upper = w2 * test_upper + (1 - w2) * test_lower\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "submission_df.head()\n",
        "\n",
        "\n",
        "# 建立提交檔\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_encoded['id'],  # 必須與 sample_submission 對齊\n",
        "    'pi_lower': final_lower,\n",
        "    'pi_upper': final_upper\n",
        "})\n",
        "\n",
        "# 輸出成 CSV\n",
        "submission_df.to_csv('xgb_predict2.csv', index=False)\n",
        "print(submission_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "輸出test LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "knn_cols = ['latitude', 'longitude'] + [col for col in X.columns if col.startswith('zoning_group_')]\n",
        "\n",
        "for col in knn_cols:\n",
        "    if col not in test_encoded.columns:\n",
        "        test_encoded[col] = 0\n",
        "\n",
        "knn_model = KNeighborsRegressor(n_neighbors=15, weights='distance')\n",
        "knn_model.fit(X[knn_cols], y)\n",
        "\n",
        "test_encoded['knn_price'] = knn_model.predict(test_encoded[knn_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "#填補缺漏欄位（對齊訓練集欄位）\n",
        "missing_cols = set(X.columns) - set(test_encoded.columns)\n",
        "for col in missing_cols:\n",
        "    test_encoded[col] = 0\n",
        "\n",
        "# 確保欄位順序一致\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4219\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 105\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 30 dense feature groups (6.10 MB) transferred to GPU in 0.005310 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 4219\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 105\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 30 dense feature groups (6.10 MB) transferred to GPU in 0.005597 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1434006.000000\n"
          ]
        }
      ],
      "source": [
        "model_lower = models[\"lower\"]\n",
        "model_upper = models[\"upper\"]\n",
        "\n",
        "final_model_lower = clone(model_lower).fit(X, y)\n",
        "final_model_upper = clone(model_upper).fit(X, y)\n",
        "\n",
        "'''test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = best_weight * test_lower\n",
        "final_upper = best_weight * test_upper\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)'''\n",
        "\n",
        "\n",
        "test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = w1 * test_lower + (1 - w1) * test_upper\n",
        "final_upper = w2 * test_upper + (1 - w2) * test_lower\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)  # optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "submission_df.head()\n",
        "test_encoded['id'] = test_df['id']  # 這行先補上 id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       id       pi_lower      pi_upper\n",
            "0  200000  810711.497651  1.115071e+06\n",
            "1  200001  579579.106918  7.605256e+05\n",
            "2  200002  452341.251537  7.000233e+05\n",
            "3  200003  334000.297807  4.349514e+05\n",
            "4  200004  425286.025519  6.336247e+05\n"
          ]
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'id': test_encoded['id'],  # 必須與 sample_submission 對齊\n",
        "    'pi_lower': final_lower,\n",
        "    'pi_upper': final_upper\n",
        "})\n",
        "\n",
        "# 輸出成 CSV\n",
        "submission_df.to_csv('lgbm_predict_2.csv', index=False)\n",
        "print(submission_df.head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_tensor_evn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
