{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\dataset.csv\"\n",
        "test_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\test.csv\"\n",
        "sam_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\sample_submission.csv\"\n",
        "\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000, 47)\n",
            "Index(['id', 'sale_date', 'sale_price', 'sale_nbr', 'sale_warning',\n",
            "       'join_status', 'join_year', 'latitude', 'longitude', 'area', 'city',\n",
            "       'zoning', 'subdivision', 'present_use', 'land_val', 'imp_val',\n",
            "       'year_built', 'year_reno', 'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
            "       'grade', 'fbsmt_grade', 'condition', 'stories', 'beds', 'bath_full',\n",
            "       'bath_3qtr', 'bath_half', 'garb_sqft', 'gara_sqft', 'wfnt', 'golf',\n",
            "       'greenbelt', 'noise_traffic', 'view_rainier', 'view_olympics',\n",
            "       'view_cascades', 'view_territorial', 'view_skyline', 'view_sound',\n",
            "       'view_lakewash', 'view_lakesamm', 'view_otherwater', 'view_other',\n",
            "       'submarket'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sale_date</th>\n",
              "      <th>sale_price</th>\n",
              "      <th>sale_nbr</th>\n",
              "      <th>sale_warning</th>\n",
              "      <th>join_status</th>\n",
              "      <th>join_year</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>area</th>\n",
              "      <th>...</th>\n",
              "      <th>view_olympics</th>\n",
              "      <th>view_cascades</th>\n",
              "      <th>view_territorial</th>\n",
              "      <th>view_skyline</th>\n",
              "      <th>view_sound</th>\n",
              "      <th>view_lakewash</th>\n",
              "      <th>view_lakesamm</th>\n",
              "      <th>view_otherwater</th>\n",
              "      <th>view_other</th>\n",
              "      <th>submarket</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-15</td>\n",
              "      <td>236000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.2917</td>\n",
              "      <td>-122.3658</td>\n",
              "      <td>53</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1999-01-15</td>\n",
              "      <td>313300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.6531</td>\n",
              "      <td>-122.1996</td>\n",
              "      <td>74</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2006-08-15</td>\n",
              "      <td>341000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4733</td>\n",
              "      <td>-122.1901</td>\n",
              "      <td>30</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1999-12-15</td>\n",
              "      <td>267000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4739</td>\n",
              "      <td>-122.3295</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-15</td>\n",
              "      <td>1650000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>miss99</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.7516</td>\n",
              "      <td>-122.1222</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 47 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id   sale_date  sale_price  sale_nbr sale_warning join_status  join_year  \\\n",
              "0   0  2014-11-15      236000       2.0                    nochg       2025   \n",
              "1   1  1999-01-15      313300       NaN          26        nochg       2025   \n",
              "2   2  2006-08-15      341000       1.0                    nochg       2025   \n",
              "3   3  1999-12-15      267000       1.0                    nochg       2025   \n",
              "4   4  2018-07-15     1650000       2.0                   miss99       2025   \n",
              "\n",
              "   latitude  longitude  area  ... view_olympics view_cascades  \\\n",
              "0   47.2917  -122.3658    53  ...             0             0   \n",
              "1   47.6531  -122.1996    74  ...             0             0   \n",
              "2   47.4733  -122.1901    30  ...             0             0   \n",
              "3   47.4739  -122.3295    96  ...             0             0   \n",
              "4   47.7516  -122.1222    36  ...             0             0   \n",
              "\n",
              "  view_territorial  view_skyline  view_sound  view_lakewash  view_lakesamm  \\\n",
              "0                0             0           0              0              0   \n",
              "1                0             0           0              1              0   \n",
              "2                0             0           0              0              0   \n",
              "3                0             0           0              0              0   \n",
              "4                0             0           0              0              0   \n",
              "\n",
              "   view_otherwater  view_other  submarket  \n",
              "0                0           0          I  \n",
              "1                0           0          Q  \n",
              "2                0           0          K  \n",
              "3                0           0          G  \n",
              "4                0           0          P  \n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "sample_df = pd.read_csv(sam_path)\n",
        "\n",
        "print(train_df.shape)\n",
        "print(train_df.columns)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sale_nbr       42182\n",
              "subdivision    17550\n",
              "submarket       1717\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#計算缺失值\n",
        "missing_values = train_df.isnull().sum()\n",
        "missing_values[missing_values > 0].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 train_encoded\n",
        "processed_cols = []\n",
        "train_encoded = pd.DataFrame()\n",
        "\n",
        "# 一、One-hot 編碼處理欄位\n",
        "confirmed_one_hot = [\n",
        "    'join_status', 'condition', 'stories', 'grade', 'fbsmt_grade', 'present_use'\n",
        "]\n",
        "onehot_df = pd.get_dummies(train_df[confirmed_one_hot], drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, onehot_df], axis=1)\n",
        "processed_cols += confirmed_one_hot\n",
        "\n",
        "# 二、處理日期特徵 + 拆解年月與季節\n",
        "train_df['sale_date'] = pd.to_datetime(train_df['sale_date'], errors='coerce')\n",
        "train_encoded['sale_year'] = train_df['sale_date'].dt.year\n",
        "train_encoded['sale_month'] = train_df['sale_date'].dt.month\n",
        "train_encoded['sale_season'] = ((train_encoded['sale_month'] - 1) // 3 + 1)\n",
        "processed_cols += ['sale_date']\n",
        "\n",
        "# 三、原始數值直接加入\n",
        "direct_add_cols = [\n",
        "    'id', 'sale_price', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half', 'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'  to much missing value\n",
        "]\n",
        "for col in direct_add_cols:\n",
        "    train_encoded[col] = train_df[col]\n",
        "processed_cols += direct_add_cols\n",
        "\n",
        "# 四、統整城市、市場與銷售警告資訊\n",
        "top_cities = train_df['city'].value_counts().nlargest(10).index.tolist()\n",
        "top_supermarket = train_df['submarket'].value_counts().nlargest(10).index.tolist()\n",
        "top_sale_warning = train_df['sale_warning'].value_counts().nlargest(15).index.tolist()\n",
        "\n",
        "train_encoded['city_simplified'] = train_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "train_encoded['submarket_simplified'] = train_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "train_encoded['sale_warning_simplified'] = train_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "\n",
        "city_dummy = pd.get_dummies(train_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "submarket_dummy = pd.get_dummies(train_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "sale_warning_dummy = pd.get_dummies(train_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, city_dummy, submarket_dummy, sale_warning_dummy], axis=1)\n",
        "processed_cols += ['city', 'submarket', 'sale_warning']\n",
        "\n",
        "# 五、Zoning 群組分類\n",
        "def zoning_group_classify(z):\n",
        "    if pd.isna(z): return 'other'\n",
        "    z = z.upper()\n",
        "    if 'SF' in z: return 'SF'\n",
        "    elif 'MR' in z: return 'MR'\n",
        "    elif 'NC' in z: return 'NC'\n",
        "    #elif 'HR' in z: return 'HR'\n",
        "    elif 'IG' in z: return 'IG'\n",
        "    elif 'P' in z: return 'P'\n",
        "    return 'other'\n",
        "\n",
        "train_encoded['zoning_group'] = train_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(train_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, zoning_dummy], axis=1)\n",
        "train_encoded.drop(columns=['zoning_group'], inplace=True)\n",
        "processed_cols += ['zoning']\n",
        "\n",
        "# 六、碎片化資訊統整成新欄位\n",
        "train_encoded['age'] = train_encoded['sale_year'] - train_encoded['year_built']\n",
        "train_encoded['renovated'] = np.where(train_encoded['year_reno'] > 0, 1, 0)\n",
        "train_encoded['years_since_reno'] = np.where(train_encoded['renovated'], train_encoded['sale_year'] - train_encoded['year_reno'], 0)\n",
        "train_encoded['total_baths'] = train_encoded['bath_full'] + 0.75 * train_encoded['bath_3qtr'] + 0.5 * train_encoded['bath_half']\n",
        "train_encoded['total_value'] = train_encoded['land_val'] + train_encoded['imp_val']\n",
        "train_encoded['living_area'] = train_encoded['sqft'] + train_encoded['sqft_fbsmt']\n",
        "\n",
        "# 七、刪除用完的簡化文字類欄位\n",
        "for col in ['city_simplified', 'submarket_simplified', 'sale_warning_simplified']:\n",
        "    train_encoded.drop(columns=[col], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "direct_add_cols = [\n",
        "    'id', 'join_year', 'latitude', 'longitude',\n",
        "    'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "    'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "    'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "\n",
        "    'bath_full', 'bath_3qtr', 'bath_half',\n",
        "    'wfnt', 'noise_traffic',\n",
        "    'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "    'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "    'view_otherwater', 'view_other'\n",
        "    #'subdivision','sale_nbr'沒有做這個 用意不大\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 建立 test_encoded 空表\n",
        "test_encoded = pd.DataFrame()\n",
        "\n",
        "# 1. One-hot 欄位\n",
        "test_onehot_df = pd.get_dummies(test_df[confirmed_one_hot], drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, test_onehot_df], axis=1)\n",
        "\n",
        "# 2. 日期處理\n",
        "test_encoded['sale_date'] = pd.to_datetime(test_df['sale_date'], errors='coerce')\n",
        "test_encoded['sale_year'] = test_encoded['sale_date'].dt.year\n",
        "test_encoded['sale_month'] = test_encoded['sale_date'].dt.month\n",
        "test_encoded['sale_season'] = ((test_encoded['sale_month'] - 1) // 3 + 1)\n",
        "test_encoded.drop(columns=['sale_date'], inplace=True)\n",
        "\n",
        "# 3. 加入 direct_add_cols 欄位\n",
        "for col in direct_add_cols:\n",
        "    test_encoded[col] = test_df[col]\n",
        "\n",
        "# 4. city / submarket / sale_warning (simplified)\n",
        "test_encoded['city_simplified'] = test_df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "city_dummy = pd.get_dummies(test_encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, city_dummy], axis=1)\n",
        "\n",
        "test_encoded['submarket_simplified'] = test_df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "submarket_dummy = pd.get_dummies(test_encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, submarket_dummy], axis=1)\n",
        "\n",
        "test_encoded['sale_warning_simplified'] = test_df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "sale_warning_dummy = pd.get_dummies(test_encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, sale_warning_dummy], axis=1)\n",
        "\n",
        "# 5. Zoning 分群 One-hot\n",
        "test_encoded['zoning_group'] = test_df['zoning'].apply(zoning_group_classify)\n",
        "zoning_dummy = pd.get_dummies(test_encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, zoning_dummy], axis=1)\n",
        "test_encoded.drop(columns=['zoning_group', 'city_simplified', 'submarket_simplified', 'sale_warning_simplified'], inplace=True)\n",
        "\n",
        "\n",
        "#碎片化資訊統整成新欄位\n",
        "test_encoded['age'] = test_encoded['sale_year'] - test_encoded['year_built']\n",
        "test_encoded['renovated'] = np.where(test_encoded['year_reno'] > 0, 1, 0)\n",
        "test_encoded['years_since_reno'] = np.where(test_encoded['renovated'], test_encoded['sale_year'] - test_encoded['year_reno'], 0)\n",
        "test_encoded['total_baths'] = test_encoded['bath_full'] + 0.75 * test_encoded['bath_3qtr'] + 0.5 * test_encoded['bath_half']\n",
        "test_encoded['total_value'] = test_encoded['land_val'] + test_encoded['imp_val']\n",
        "test_encoded['living_area'] = test_encoded['sqft'] + test_encoded['sqft_fbsmt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "bool       52\n",
              "int64      40\n",
              "float64     4\n",
              "int32       4\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#確認資料類型\n",
        "train_encoded.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model(XGBoost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 建立上下界\n",
        "y_lower = y * 0.9\n",
        "y_upper = y * 1.1\n",
        "\n",
        "# 拆分資料\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "y_lower_train = y_train * 0.9\n",
        "y_lower_val = y_val * 0.9\n",
        "y_upper_train = y_train * 1.1\n",
        "y_upper_val = y_val * 1.1\n",
        "# 模型 A：下限\n",
        "model_lower = xgb.XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,early_stopping_rounds=20,eval_metric='rmse',tree_method='gpu_hist')\n",
        "\n",
        "# 模型 B：上限\n",
        "model_upper = xgb.XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.01,early_stopping_rounds=20,eval_metric='rmse',tree_method='gpu_hist')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#下限預測\n",
        "model_lower.fit(X=X_train,\n",
        "                y=y_lower_train,\n",
        "                eval_set=[(X_val, y_lower_val)],\n",
        "                verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#上限預測\n",
        "model_upper.fit(X=X_train,\n",
        "                y=y_upper_train,\n",
        "                eval_set=[(X_val, y_upper_val)],\n",
        "                verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LGBM Quantile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
        "    width = upper - lower\n",
        "    below = np.maximum(lower - y_true, 0)\n",
        "    above = np.maximum(y_true - upper, 0)\n",
        "    return width + (2 / alpha) * (below + above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def oof_and_hill_climb_two_weights(X, y, model_lower, model_upper, alpha=0.1, n_splits=5, seed=42, steps=100):\n",
        "    oof_lowers = np.zeros(len(X))\n",
        "    oof_uppers = np.zeros(len(X))\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train = y.iloc[train_idx]\n",
        "\n",
        "        lower_model = clone(model_lower)\n",
        "        upper_model = clone(model_upper)\n",
        "\n",
        "        lower_model.fit(X_train, y_train-0.01)\n",
        "        upper_model.fit(X_train, y_train+0.01)\n",
        "\n",
        "        oof_lowers[val_idx] = lower_model.predict(X_val)\n",
        "        oof_uppers[val_idx] = upper_model.predict(X_val)\n",
        "\n",
        "    # 初始化雙權重\n",
        "    current_w1 = 0.425  # 下限 weight\n",
        "    current_w2 = 0.575  # 上限 weight\n",
        "\n",
        "    best_score = np.inf\n",
        "    best_weights = (current_w1, current_w2)\n",
        "\n",
        "    for step in range(steps):\n",
        "        # 微調 perturbation，讓 weight 有隨機性（避免卡住）\n",
        "        perturb1 = np.random.dirichlet([9])[0] - 0.9\n",
        "        perturb2 = np.random.dirichlet([9])[0] - 0.9\n",
        "\n",
        "        w1 = np.clip(current_w1 + 0.1 * perturb1, 0, 1)\n",
        "        w2 = np.clip(current_w2 + 0.1 * perturb2, 0, 1)\n",
        "\n",
        "        # 雙權重組合\n",
        "        lower_combined = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "        upper_combined = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "\n",
        "        # 修正：確保上下限方向正確（防止預測範圍錯位）\n",
        "        lower_combined, upper_combined = np.minimum(lower_combined, upper_combined), np.maximum(lower_combined, upper_combined)\n",
        "\n",
        "        score = np.mean(winkler_score(y, lower_combined, upper_combined, alpha))\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_weights = (w1, w2)\n",
        "            current_w1, current_w2 = w1, w2\n",
        "            print(f\"[Step {step}] ✅ Improved Score: {best_score:.2f} (w1: {w1:.4f}, w2: {w2:.4f})\")\n",
        "\n",
        "    return oof_lowers, oof_uppers, best_weights, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"lower\": lgb.LGBMRegressor(\n",
        "        objective=\"quantile\",\n",
        "        alpha=0.05,\n",
        "        device=\"cpu\",\n",
        "        n_estimators=1500,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        subsample_freq=1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"upper\": lgb.LGBMRegressor(\n",
        "        objective=\"quantile\",\n",
        "        alpha=0.95,\n",
        "        device=\"cpu\",\n",
        "        n_estimators=1500,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        subsample_freq=1,\n",
        "        random_state=42\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018651 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 184999.984375\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016148 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 1435000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017163 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 184999.984375\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015322 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016798 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 184999.984375\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016974 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 1431000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014520 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 184999.984375\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016059 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 1440000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017092 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 184999.984375\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015602 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[Step 0] ✅ Improved Score: 1220477.69 (w1: 0.4350, w2: 0.5850)\n",
            "[Step 1] ✅ Improved Score: 1183749.02 (w1: 0.4450, w2: 0.5950)\n",
            "[Step 2] ✅ Improved Score: 1147935.31 (w1: 0.4550, w2: 0.6050)\n",
            "[Step 3] ✅ Improved Score: 1113022.19 (w1: 0.4650, w2: 0.6150)\n",
            "[Step 4] ✅ Improved Score: 1079004.29 (w1: 0.4750, w2: 0.6250)\n",
            "[Step 5] ✅ Improved Score: 1045886.59 (w1: 0.4850, w2: 0.6350)\n",
            "[Step 6] ✅ Improved Score: 1013668.54 (w1: 0.4950, w2: 0.6450)\n",
            "[Step 7] ✅ Improved Score: 982353.99 (w1: 0.5050, w2: 0.6550)\n",
            "[Step 8] ✅ Improved Score: 951915.25 (w1: 0.5150, w2: 0.6650)\n",
            "[Step 9] ✅ Improved Score: 922359.43 (w1: 0.5250, w2: 0.6750)\n",
            "[Step 10] ✅ Improved Score: 893685.51 (w1: 0.5350, w2: 0.6850)\n",
            "[Step 11] ✅ Improved Score: 865875.45 (w1: 0.5450, w2: 0.6950)\n",
            "[Step 12] ✅ Improved Score: 838933.75 (w1: 0.5550, w2: 0.7050)\n",
            "[Step 13] ✅ Improved Score: 812832.17 (w1: 0.5650, w2: 0.7150)\n",
            "[Step 14] ✅ Improved Score: 787568.80 (w1: 0.5750, w2: 0.7250)\n",
            "[Step 15] ✅ Improved Score: 763146.92 (w1: 0.5850, w2: 0.7350)\n",
            "[Step 16] ✅ Improved Score: 739550.57 (w1: 0.5950, w2: 0.7450)\n",
            "[Step 17] ✅ Improved Score: 716766.34 (w1: 0.6050, w2: 0.7550)\n",
            "[Step 18] ✅ Improved Score: 694784.72 (w1: 0.6150, w2: 0.7650)\n",
            "[Step 19] ✅ Improved Score: 673600.72 (w1: 0.6250, w2: 0.7750)\n",
            "[Step 20] ✅ Improved Score: 653210.17 (w1: 0.6350, w2: 0.7850)\n",
            "[Step 21] ✅ Improved Score: 633582.71 (w1: 0.6450, w2: 0.7950)\n",
            "[Step 22] ✅ Improved Score: 614682.32 (w1: 0.6550, w2: 0.8050)\n",
            "[Step 23] ✅ Improved Score: 596552.64 (w1: 0.6650, w2: 0.8150)\n",
            "[Step 24] ✅ Improved Score: 579169.63 (w1: 0.6750, w2: 0.8250)\n",
            "[Step 25] ✅ Improved Score: 562507.22 (w1: 0.6850, w2: 0.8350)\n",
            "[Step 26] ✅ Improved Score: 546562.63 (w1: 0.6950, w2: 0.8450)\n",
            "[Step 27] ✅ Improved Score: 531332.05 (w1: 0.7050, w2: 0.8550)\n",
            "[Step 28] ✅ Improved Score: 516808.79 (w1: 0.7150, w2: 0.8650)\n",
            "[Step 29] ✅ Improved Score: 502946.67 (w1: 0.7250, w2: 0.8750)\n",
            "[Step 30] ✅ Improved Score: 489735.11 (w1: 0.7350, w2: 0.8850)\n",
            "[Step 31] ✅ Improved Score: 477174.75 (w1: 0.7450, w2: 0.8950)\n",
            "[Step 32] ✅ Improved Score: 465274.77 (w1: 0.7550, w2: 0.9050)\n",
            "[Step 33] ✅ Improved Score: 454013.29 (w1: 0.7650, w2: 0.9150)\n",
            "[Step 34] ✅ Improved Score: 443357.64 (w1: 0.7750, w2: 0.9250)\n",
            "[Step 35] ✅ Improved Score: 433294.48 (w1: 0.7850, w2: 0.9350)\n",
            "[Step 36] ✅ Improved Score: 423826.19 (w1: 0.7950, w2: 0.9450)\n",
            "[Step 37] ✅ Improved Score: 414940.50 (w1: 0.8050, w2: 0.9550)\n",
            "[Step 38] ✅ Improved Score: 406604.22 (w1: 0.8150, w2: 0.9650)\n",
            "[Step 39] ✅ Improved Score: 398808.26 (w1: 0.8250, w2: 0.9750)\n",
            "[Step 40] ✅ Improved Score: 391529.33 (w1: 0.8350, w2: 0.9850)\n",
            "[Step 41] ✅ Improved Score: 384756.09 (w1: 0.8450, w2: 0.9950)\n",
            "[Step 42] ✅ Improved Score: 379229.35 (w1: 0.8550, w2: 1.0000)\n",
            "[Step 43] ✅ Improved Score: 374808.27 (w1: 0.8650, w2: 1.0000)\n",
            "[Step 44] ✅ Improved Score: 370684.87 (w1: 0.8750, w2: 1.0000)\n",
            "[Step 45] ✅ Improved Score: 366859.11 (w1: 0.8850, w2: 1.0000)\n",
            "[Step 46] ✅ Improved Score: 363340.94 (w1: 0.8950, w2: 1.0000)\n",
            "[Step 47] ✅ Improved Score: 360098.40 (w1: 0.9050, w2: 1.0000)\n",
            "[Step 48] ✅ Improved Score: 357125.11 (w1: 0.9150, w2: 1.0000)\n",
            "[Step 49] ✅ Improved Score: 354406.92 (w1: 0.9250, w2: 1.0000)\n",
            "[Step 50] ✅ Improved Score: 351921.32 (w1: 0.9350, w2: 1.0000)\n",
            "[Step 51] ✅ Improved Score: 349685.80 (w1: 0.9450, w2: 1.0000)\n",
            "[Step 52] ✅ Improved Score: 347703.04 (w1: 0.9550, w2: 1.0000)\n",
            "[Step 53] ✅ Improved Score: 345956.19 (w1: 0.9650, w2: 1.0000)\n",
            "[Step 54] ✅ Improved Score: 344437.57 (w1: 0.9750, w2: 1.0000)\n",
            "[Step 55] ✅ Improved Score: 343131.07 (w1: 0.9850, w2: 1.0000)\n",
            "[Step 56] ✅ Improved Score: 342032.47 (w1: 0.9950, w2: 1.0000)\n",
            "[Step 57] ✅ Improved Score: 341552.73 (w1: 1.0000, w2: 1.0000)\n"
          ]
        }
      ],
      "source": [
        "oof_lowers, oof_uppers, (w1, w2), best_score = oof_and_hill_climb_two_weights(\n",
        "    X, y,\n",
        "    model_lower=models[\"lower\"],\n",
        "    model_upper=models[\"upper\"],\n",
        "    alpha=0.1, \n",
        "    n_splits=5,\n",
        "    steps=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "輸出test xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#填補缺漏欄位（對齊訓練集欄位）\n",
        "missing_cols = set(X_train.columns) - set(test_encoded.columns)\n",
        "for col in missing_cols:\n",
        "    test_encoded[col] = 0\n",
        "\n",
        "# 確保欄位順序一致\n",
        "test_encoded = test_encoded[X_train.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "set()\n"
          ]
        }
      ],
      "source": [
        "print(set(X_train.columns) - set(test_encoded.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>pi_lower</th>\n",
              "      <th>pi_upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200000</td>\n",
              "      <td>0</td>\n",
              "      <td>100000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>200001</td>\n",
              "      <td>0</td>\n",
              "      <td>100000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>200002</td>\n",
              "      <td>0</td>\n",
              "      <td>100000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>200003</td>\n",
              "      <td>0</td>\n",
              "      <td>100000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>200004</td>\n",
              "      <td>0</td>\n",
              "      <td>100000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  pi_lower   pi_upper\n",
              "0  200000         0  100000000\n",
              "1  200001         0  100000000\n",
              "2  200002         0  100000000\n",
              "3  200003         0  100000000\n",
              "4  200004         0  100000000"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "submission_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_encoded['id'] = test_df['id']  # 這行先補上 id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 預測上下限\n",
        "y_lower_pred = model_lower.predict(test_encoded.drop(columns=['id']))\n",
        "y_upper_pred = model_upper.predict(test_encoded.drop(columns=['id']))\n",
        "\n",
        "# 建立提交檔\n",
        "submission_df = pd.DataFrame({\n",
        "    'id': test_encoded['id'],  # 必須與 sample_submission 對齊\n",
        "    'pi_lower': y_lower_pred,\n",
        "    'pi_upper': y_upper_pred\n",
        "})\n",
        "\n",
        "# 輸出成 CSV\n",
        "submission_df.to_csv('xgb_predict.csv', index=False)\n",
        "print(submission_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "輸出test LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#填補缺漏欄位（對齊訓練集欄位）\n",
        "missing_cols = set(X.columns) - set(test_encoded.columns)\n",
        "for col in missing_cols:\n",
        "    test_encoded[col] = 0\n",
        "\n",
        "# 確保欄位順序一致\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.005452 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3845\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004503 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1435000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004989 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004292 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004324 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3843\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003735 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1431000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004233 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3836\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003492 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1440000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.004044 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3838\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (4.27 MB) transferred to GPU in 0.003498 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1430000.000000\n",
            "[Step 0] Improved MWIS: 6132346.42 (Weight: 0.4100)\n",
            "[Step 1] Improved MWIS: 5998138.12 (Weight: 0.4200)\n",
            "[Step 2] Improved MWIS: 5864083.19 (Weight: 0.4300)\n",
            "[Step 3] Improved MWIS: 5730199.30 (Weight: 0.4400)\n",
            "[Step 4] Improved MWIS: 5596511.15 (Weight: 0.4500)\n",
            "[Step 5] Improved MWIS: 5463018.77 (Weight: 0.4600)\n",
            "[Step 6] Improved MWIS: 5329716.49 (Weight: 0.4700)\n",
            "[Step 7] Improved MWIS: 5196602.32 (Weight: 0.4800)\n",
            "[Step 8] Improved MWIS: 5063747.28 (Weight: 0.4900)\n",
            "[Step 9] Improved MWIS: 4931168.66 (Weight: 0.5000)\n",
            "[Step 10] Improved MWIS: 4798872.28 (Weight: 0.5100)\n",
            "[Step 11] Improved MWIS: 4666890.87 (Weight: 0.5200)\n",
            "[Step 12] Improved MWIS: 4535241.47 (Weight: 0.5300)\n",
            "[Step 13] Improved MWIS: 4403949.01 (Weight: 0.5400)\n",
            "[Step 14] Improved MWIS: 4273021.68 (Weight: 0.5500)\n",
            "[Step 15] Improved MWIS: 4142466.76 (Weight: 0.5600)\n",
            "[Step 16] Improved MWIS: 4012315.06 (Weight: 0.5700)\n",
            "[Step 17] Improved MWIS: 3882623.63 (Weight: 0.5800)\n",
            "[Step 18] Improved MWIS: 3753424.52 (Weight: 0.5900)\n",
            "[Step 19] Improved MWIS: 3624740.01 (Weight: 0.6000)\n",
            "[Step 20] Improved MWIS: 3496604.74 (Weight: 0.6100)\n",
            "[Step 21] Improved MWIS: 3369087.47 (Weight: 0.6200)\n",
            "[Step 22] Improved MWIS: 3242267.39 (Weight: 0.6300)\n",
            "[Step 23] Improved MWIS: 3116209.35 (Weight: 0.6400)\n",
            "[Step 24] Improved MWIS: 2991022.61 (Weight: 0.6500)\n",
            "[Step 25] Improved MWIS: 2866738.68 (Weight: 0.6600)\n",
            "[Step 26] Improved MWIS: 2743466.81 (Weight: 0.6700)\n",
            "[Step 27] Improved MWIS: 2621243.19 (Weight: 0.6800)\n",
            "[Step 28] Improved MWIS: 2500209.55 (Weight: 0.6900)\n",
            "[Step 29] Improved MWIS: 2380487.12 (Weight: 0.7000)\n",
            "[Step 30] Improved MWIS: 2262178.94 (Weight: 0.7100)\n",
            "[Step 31] Improved MWIS: 2145526.51 (Weight: 0.7200)\n",
            "[Step 32] Improved MWIS: 2030689.68 (Weight: 0.7300)\n",
            "[Step 33] Improved MWIS: 1917840.75 (Weight: 0.7400)\n",
            "[Step 34] Improved MWIS: 1807139.16 (Weight: 0.7500)\n",
            "[Step 35] Improved MWIS: 1698786.24 (Weight: 0.7600)\n",
            "[Step 36] Improved MWIS: 1593036.82 (Weight: 0.7700)\n",
            "[Step 37] Improved MWIS: 1490133.56 (Weight: 0.7800)\n",
            "[Step 38] Improved MWIS: 1390254.94 (Weight: 0.7900)\n",
            "[Step 39] Improved MWIS: 1293675.49 (Weight: 0.8000)\n",
            "[Step 40] Improved MWIS: 1200790.81 (Weight: 0.8100)\n",
            "[Step 41] Improved MWIS: 1111796.51 (Weight: 0.8200)\n",
            "[Step 42] Improved MWIS: 1026969.04 (Weight: 0.8300)\n",
            "[Step 43] Improved MWIS: 946610.10 (Weight: 0.8400)\n",
            "[Step 44] Improved MWIS: 870883.14 (Weight: 0.8500)\n",
            "[Step 45] Improved MWIS: 800085.84 (Weight: 0.8600)\n",
            "[Step 46] Improved MWIS: 734358.53 (Weight: 0.8700)\n",
            "[Step 47] Improved MWIS: 673869.19 (Weight: 0.8800)\n",
            "[Step 48] Improved MWIS: 618718.30 (Weight: 0.8900)\n",
            "[Step 49] Improved MWIS: 569060.83 (Weight: 0.9000)\n",
            "[Step 50] Improved MWIS: 524864.35 (Weight: 0.9100)\n",
            "[Step 51] Improved MWIS: 486048.75 (Weight: 0.9200)\n",
            "[Step 52] Improved MWIS: 452383.38 (Weight: 0.9300)\n",
            "[Step 53] Improved MWIS: 423677.89 (Weight: 0.9400)\n",
            "[Step 54] Improved MWIS: 399765.80 (Weight: 0.9500)\n",
            "[Step 55] Improved MWIS: 380256.93 (Weight: 0.9600)\n",
            "[Step 56] Improved MWIS: 364930.36 (Weight: 0.9700)\n",
            "[Step 57] Improved MWIS: 353482.91 (Weight: 0.9800)\n",
            "[Step 58] Improved MWIS: 345674.64 (Weight: 0.9900)\n",
            "[Step 59] Improved MWIS: 341379.28 (Weight: 1.0000)\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3856\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (5.34 MB) transferred to GPU in 0.005645 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 185000.000000\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3856\n",
            "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 97\n",
            "[LightGBM] [Info] Using GPU Device: Intel(R) UHD Graphics 630, Vendor: Intel(R) Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 27 dense feature groups (5.34 MB) transferred to GPU in 0.004793 secs. 1 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 1434006.000000\n"
          ]
        }
      ],
      "source": [
        "model_lower = models[\"lower\"]\n",
        "model_upper = models[\"upper\"]\n",
        "\n",
        "final_model_lower = clone(model_lower).fit(X, y)\n",
        "final_model_upper = clone(model_upper).fit(X, y)\n",
        "\n",
        "'''test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = best_weight * test_lower\n",
        "final_upper = best_weight * test_upper\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)'''\n",
        "\n",
        "\n",
        "test_lower = final_model_lower.predict(test_encoded)\n",
        "test_upper = final_model_upper.predict(test_encoded)\n",
        "\n",
        "final_lower = w1 * test_lower + (1 - w1) * test_upper\n",
        "final_upper = w2 * test_upper + (1 - w2) * test_lower\n",
        "\n",
        "final_lower, final_upper = np.minimum(final_lower, final_upper), np.maximum(final_lower, final_upper)\n",
        "final_lower = np.maximum(final_lower, 0)  # optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "submission_df.head()\n",
        "test_encoded['id'] = test_df['id']  # 這行先補上 id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       id       pi_lower      pi_upper\n",
            "0  200000  823500.777056  1.125383e+06\n",
            "1  200001  590158.900174  7.270068e+05\n",
            "2  200002  469229.953050  6.502398e+05\n",
            "3  200003  315274.697956  4.363184e+05\n",
            "4  200004  413919.532224  6.498657e+05\n"
          ]
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'id': test_encoded['id'],  # 必須與 sample_submission 對齊\n",
        "    'pi_lower': final_lower,\n",
        "    'pi_upper': final_upper\n",
        "})\n",
        "\n",
        "# 輸出成 CSV\n",
        "submission_df.to_csv('lgbm_predict.csv', index=False)\n",
        "print(submission_df.head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_tensor_evn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
