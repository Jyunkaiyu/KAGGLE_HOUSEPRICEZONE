{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import geohash2 as geohash\n",
        "\n",
        "\n",
        "train_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\dataset.csv\"\n",
        "test_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\test.csv\"\n",
        "sam_path = \"C:\\\\Users\\\\USER\\\\Desktop\\\\House price\\\\sample_submission.csv\"\n",
        "\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000, 47)\n",
            "Index(['id', 'sale_date', 'sale_price', 'sale_nbr', 'sale_warning',\n",
            "       'join_status', 'join_year', 'latitude', 'longitude', 'area', 'city',\n",
            "       'zoning', 'subdivision', 'present_use', 'land_val', 'imp_val',\n",
            "       'year_built', 'year_reno', 'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
            "       'grade', 'fbsmt_grade', 'condition', 'stories', 'beds', 'bath_full',\n",
            "       'bath_3qtr', 'bath_half', 'garb_sqft', 'gara_sqft', 'wfnt', 'golf',\n",
            "       'greenbelt', 'noise_traffic', 'view_rainier', 'view_olympics',\n",
            "       'view_cascades', 'view_territorial', 'view_skyline', 'view_sound',\n",
            "       'view_lakewash', 'view_lakesamm', 'view_otherwater', 'view_other',\n",
            "       'submarket'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sale_date</th>\n",
              "      <th>sale_price</th>\n",
              "      <th>sale_nbr</th>\n",
              "      <th>sale_warning</th>\n",
              "      <th>join_status</th>\n",
              "      <th>join_year</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>area</th>\n",
              "      <th>...</th>\n",
              "      <th>view_olympics</th>\n",
              "      <th>view_cascades</th>\n",
              "      <th>view_territorial</th>\n",
              "      <th>view_skyline</th>\n",
              "      <th>view_sound</th>\n",
              "      <th>view_lakewash</th>\n",
              "      <th>view_lakesamm</th>\n",
              "      <th>view_otherwater</th>\n",
              "      <th>view_other</th>\n",
              "      <th>submarket</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-15</td>\n",
              "      <td>236000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.2917</td>\n",
              "      <td>-122.3658</td>\n",
              "      <td>53</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1999-01-15</td>\n",
              "      <td>313300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26</td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.6531</td>\n",
              "      <td>-122.1996</td>\n",
              "      <td>74</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2006-08-15</td>\n",
              "      <td>341000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4733</td>\n",
              "      <td>-122.1901</td>\n",
              "      <td>30</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1999-12-15</td>\n",
              "      <td>267000</td>\n",
              "      <td>1.0</td>\n",
              "      <td></td>\n",
              "      <td>nochg</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.4739</td>\n",
              "      <td>-122.3295</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-15</td>\n",
              "      <td>1650000</td>\n",
              "      <td>2.0</td>\n",
              "      <td></td>\n",
              "      <td>miss99</td>\n",
              "      <td>2025</td>\n",
              "      <td>47.7516</td>\n",
              "      <td>-122.1222</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 47 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id   sale_date  sale_price  sale_nbr sale_warning join_status  join_year  \\\n",
              "0   0  2014-11-15      236000       2.0                    nochg       2025   \n",
              "1   1  1999-01-15      313300       NaN          26        nochg       2025   \n",
              "2   2  2006-08-15      341000       1.0                    nochg       2025   \n",
              "3   3  1999-12-15      267000       1.0                    nochg       2025   \n",
              "4   4  2018-07-15     1650000       2.0                   miss99       2025   \n",
              "\n",
              "   latitude  longitude  area  ... view_olympics view_cascades  \\\n",
              "0   47.2917  -122.3658    53  ...             0             0   \n",
              "1   47.6531  -122.1996    74  ...             0             0   \n",
              "2   47.4733  -122.1901    30  ...             0             0   \n",
              "3   47.4739  -122.3295    96  ...             0             0   \n",
              "4   47.7516  -122.1222    36  ...             0             0   \n",
              "\n",
              "  view_territorial  view_skyline  view_sound  view_lakewash  view_lakesamm  \\\n",
              "0                0             0           0              0              0   \n",
              "1                0             0           0              1              0   \n",
              "2                0             0           0              0              0   \n",
              "3                0             0           0              0              0   \n",
              "4                0             0           0              0              0   \n",
              "\n",
              "   view_otherwater  view_other  submarket  \n",
              "0                0           0          I  \n",
              "1                0           0          Q  \n",
              "2                0           0          K  \n",
              "3                0           0          G  \n",
              "4                0           0          P  \n",
              "\n",
              "[5 rows x 47 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "sample_df = pd.read_csv(sam_path)\n",
        "\n",
        "print(train_df.shape)\n",
        "print(train_df.columns)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sale_nbr       42182\n",
              "subdivision    17550\n",
              "submarket       1717\n",
              "dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#計算缺失值\n",
        "missing_values = train_df.isnull().sum()\n",
        "missing_values[missing_values > 0].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def zoning_group_classify(z):\n",
        "    if pd.isna(z): return 'other'\n",
        "    z = z.upper()\n",
        "    if 'SF' in z: return 'SF'\n",
        "    elif 'MR' in z: return 'MR'\n",
        "    elif 'NC' in z: return 'NC'\n",
        "    elif 'P' in z: return 'P'\n",
        "    elif 'HR' in z or 'IG' in z: return 'other'\n",
        "    return 'other'\n",
        "\n",
        "def encode_dataset(df, is_train=True, top_cities=None, top_supermarket=None, top_sale_warning=None):\n",
        "    \n",
        "    encoded = pd.DataFrame()\n",
        "    confirmed_one_hot = ['join_status', 'condition', 'stories', 'grade', 'fbsmt_grade', 'present_use']\n",
        "\n",
        "    direct_add_cols = [\n",
        "        'id', 'sale_price', 'join_year', 'latitude', 'longitude',\n",
        "        'area', 'land_val', 'imp_val', 'year_built', 'year_reno',\n",
        "        'sqft_lot', 'sqft', 'sqft_1', 'sqft_fbsmt',\n",
        "        'beds', 'garb_sqft', 'gara_sqft', 'golf', 'greenbelt',\n",
        "        'bath_full', 'bath_3qtr', 'bath_half', 'wfnt', 'noise_traffic',\n",
        "        'view_rainier', 'view_olympics', 'view_cascades', 'view_territorial',\n",
        "        'view_skyline', 'view_sound', 'view_lakewash', 'view_lakesamm',\n",
        "        'view_otherwater', 'view_other'\n",
        "        #'subdivision','sale_nbr'沒有做這個 用意不大\n",
        "    ]\n",
        "\n",
        "    onehot_df = pd.get_dummies(df[confirmed_one_hot], drop_first=False)\n",
        "    encoded = pd.concat([encoded, onehot_df], axis=1)\n",
        "\n",
        "    df['sale_date'] = pd.to_datetime(df['sale_date'], errors='coerce')\n",
        "    encoded['sale_year'] = df['sale_date'].dt.year\n",
        "    encoded['sale_month'] = df['sale_date'].dt.month\n",
        "    encoded['sale_season'] = ((encoded['sale_month'] - 1) // 3 + 1)\n",
        "\n",
        "    for col in direct_add_cols:\n",
        "        if col in df.columns:\n",
        "            encoded[col] = df[col]\n",
        "\n",
        "    if is_train:\n",
        "        top_cities = df['city'].value_counts().nlargest(10).index.tolist()\n",
        "        top_supermarket = df['submarket'].value_counts().nlargest(10).index.tolist()\n",
        "        top_sale_warning = df['sale_warning'].value_counts().nlargest(15).index.tolist()\n",
        "\n",
        "    encoded['city_simplified'] = df['city'].apply(lambda x: x if x in top_cities else 'other')\n",
        "    encoded['submarket_simplified'] = df['submarket'].apply(lambda x: x if x in top_supermarket else 'other')\n",
        "    encoded['sale_warning_simplified'] = df['sale_warning'].apply(lambda x: x if x in top_sale_warning else 'other')\n",
        "\n",
        "    city_dummy = pd.get_dummies(encoded['city_simplified'], prefix='city', drop_first=False)\n",
        "    submarket_dummy = pd.get_dummies(encoded['submarket_simplified'], prefix='submarket', drop_first=False)\n",
        "    sale_warning_dummy = pd.get_dummies(encoded['sale_warning_simplified'], prefix='sale_warning', drop_first=False)\n",
        "    encoded = pd.concat([encoded, city_dummy, submarket_dummy, sale_warning_dummy], axis=1)\n",
        "\n",
        "    encoded['zoning_group'] = df['zoning'].apply(zoning_group_classify)\n",
        "    zoning_dummy = pd.get_dummies(encoded['zoning_group'], prefix='zoning_group', drop_first=False)\n",
        "    encoded = pd.concat([encoded, zoning_dummy], axis=1)\n",
        "    encoded.drop(columns=['zoning_group', 'city_simplified', 'submarket_simplified', 'sale_warning_simplified'], inplace=True)\n",
        "\n",
        "    encoded['age'] = encoded['sale_year'] - encoded['year_built']\n",
        "    encoded['renovated'] = np.where(encoded['year_reno'] > 0, 1, 0)\n",
        "    encoded['years_since_reno'] = np.where(encoded['renovated'], encoded['sale_year'] - encoded['year_reno'], 0)\n",
        "    encoded['total_baths'] = encoded['bath_full'] + 0.75 * encoded['bath_3qtr'] + 0.5 * encoded['bath_half']\n",
        "    encoded['total_value'] = encoded['land_val'] + encoded['imp_val']\n",
        "    encoded['living_area'] = encoded['sqft'] + encoded['sqft_fbsmt']\n",
        "\n",
        "\n",
        "    encoded[\"floor_ratio\"] = np.where(\n",
        "    encoded[\"sqft_lot\"] == 0,\n",
        "    0,\n",
        "    encoded[\"sqft\"] / encoded[\"sqft_lot\"]\n",
        "    )\n",
        "\n",
        "    encoded[\"is_large_house\"] = (encoded[\"sqft\"] > 3000).astype(int)\n",
        "    encoded[\"is_recent_reno\"] = (encoded[\"years_since_reno\"] <= 5).astype(int)\n",
        "    encoded[\"bath_per_bed\"] = encoded[\"total_baths\"] / encoded[\"beds\"]\n",
        "    encoded[\"bath_per_bed\"] = encoded[\"bath_per_bed\"].replace([np.inf, -np.inf], 0).fillna(0)\n",
        "    \n",
        "    return encoded, top_cities, top_supermarket, top_sale_warning\n",
        "\n",
        "'''def pca_train_test(encoded, feature, scaler=None, pca=None, kmeans=None):\n",
        "    \"\"\"\n",
        "    如果傳入 scaler/pca/kmeans == None → 在 df 上 fit\n",
        "    否則只做 transform / predict\n",
        "    回傳 (encoded, scaler, pca, kmeans)\n",
        "    \"\"\"\n",
        "    # 1. 標準化\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler().fit(encoded[feature])\n",
        "    X_scaled = scaler.transform(encoded[feature])\n",
        "\n",
        "    # 2. PCA\n",
        "    if pca is None:\n",
        "        pca = PCA(n_components=3, random_state=42).fit(X_scaled)\n",
        "    X_pca = pca.transform(X_scaled)\n",
        "\n",
        "    # 3. KMeans\n",
        "    if kmeans is None:\n",
        "        kmeans = KMeans(n_clusters=10, random_state=42).fit(X_pca)\n",
        "    encoded[\"pca_region_cluster\"] = kmeans.predict(X_pca)\n",
        "\n",
        "    # 4. One‑hot\n",
        "    dummies = pd.get_dummies(encoded[\"pca_region_cluster\"], prefix=\"pca_region\")\n",
        "    encoded = pd.concat([encoded, dummies], axis=1)\n",
        "    encoded.drop(columns=[\"pca_region_cluster\"], inplace=True)\n",
        "\n",
        "    return encoded, scaler, pca, kmeans'''\n",
        "\n",
        "\n",
        "\n",
        "def clean_features(df, log_cols=None, clip_cols=None, add_log_target=False, log_target_col=\"sale_price_log\"):\n",
        "    \"\"\"\n",
        "    對 df 做對數化 / clip。\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "    log_cols : list[str]  要 log1p 的欄位。預設常見數值長尾欄。\n",
        "    clip_cols : list[str] 要 clip 的欄位（可 None）。\n",
        "    add_log_target : bool  是否另外產生 log 版目標欄（不覆寫原欄位）。\n",
        "    log_target_col : str   新增目標欄名稱。\n",
        "    Returns\n",
        "    -------\n",
        "    cleaned_df : pd.DataFrame\n",
        "    \"\"\"\n",
        "    df = df.copy()  # 不汙染呼叫端\n",
        "\n",
        "    # 1️⃣ 預設欄位表\n",
        "    if log_cols is None:\n",
        "        log_cols = ['land_val', 'imp_val', 'sqft_lot',\n",
        "                    'garb_sqft', 'floor_ratio', 'total_value']\n",
        "    if clip_cols is None:\n",
        "        clip_cols = ['land_val', 'imp_val', 'sqft_lot']\n",
        "\n",
        "    # 2️⃣ log1p\n",
        "    for col in log_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.log1p(df[col])\n",
        "\n",
        "    # 3️⃣ clip（可選）\n",
        "    for col in clip_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].clip(upper=1_000_000)\n",
        "\n",
        "    # 4️⃣ 目標值也要 log（加新欄，不覆蓋）\n",
        "    if add_log_target and 'sale_price' in df.columns:\n",
        "        df[log_target_col] = np.log1p(df['sale_price'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_geohash(df, prec=6):\n",
        "    df = df.copy()\n",
        "    df['gh6'] = df.apply(\n",
        "        lambda r: geohash.encode(r.latitude, r.longitude, precision=prec),\n",
        "        axis=1\n",
        "    )\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1️⃣ 產生 base features\n",
        "train_encoded, top_cities, top_supermarket, top_sale_warning = encode_dataset(train_df, is_train=True)\n",
        "test_encoded , _ , _ , _  = encode_dataset(test_df , is_train=False,\n",
        "                                           top_cities=top_cities,\n",
        "                                           top_supermarket=top_supermarket,\n",
        "                                           top_sale_warning=top_sale_warning)\n",
        "\n",
        "'''# 2️⃣ PCA + KMeans：train 先 fit，test 只 transform\n",
        "pca_features = ['latitude', 'longitude', 'sqft', 'area', 'total_value', 'imp_val']\n",
        "train_encoded, scaler, pca, kmeans = pca_train_test(train_encoded, pca_features)\n",
        "test_encoded , _,     _,   _       = pca_train_test(test_encoded , pca_features,\n",
        "                                                    scaler=scaler, pca=pca, kmeans=kmeans)'''\n",
        "\n",
        "# geohash 6 \n",
        "train_encoded = add_geohash(train_encoded)\n",
        "test_encoded  = add_geohash(test_encoded)\n",
        "\n",
        "# ---------- TRAIN ----------\n",
        "gh_train_dum = pd.get_dummies(train_encoded['gh6'], prefix='gh6', drop_first=False)\n",
        "train_encoded = pd.concat([train_encoded, gh_train_dum], axis=1)\n",
        "train_encoded.drop(columns=['gh6'], inplace=True)    # 原 gh6 類別欄可丟\n",
        "\n",
        "# ---------- TEST ----------\n",
        "gh_test_dum = pd.get_dummies(test_encoded['gh6'], prefix='gh6', drop_first=False)\n",
        "test_encoded = pd.concat([test_encoded, gh_test_dum], axis=1)\n",
        "test_encoded.drop(columns=['gh6'], inplace=True)\n",
        "\n",
        "# 3️⃣ 其餘特徵工程\n",
        "train_encoded = clean_features(train_encoded, add_log_target=True)\n",
        "test_encoded  = clean_features(test_encoded , add_log_target=False)\n",
        "\n",
        "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "bool       3891\n",
              "int64        35\n",
              "float64      12\n",
              "int32         6\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#確認資料類型\n",
        "train_encoded.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LGBM Quantile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''# 分割訓練特徵與目標\n",
        "X = train_encoded.drop(columns=['sale_price', 'id'])  # id 可留給最後輸出\n",
        "y = train_encoded['sale_price']'''\n",
        "\n",
        "# 👉 用 log(price) 當 y\n",
        "X = train_encoded.drop(columns=['sale_price', 'sale_price_log', 'id'])\n",
        "y = train_encoded['sale_price_log']\n",
        "y_raw = np.expm1(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def winkler_score(y_true, lower, upper, alpha=0.1):\n",
        "    width = upper - lower\n",
        "    below = np.maximum(lower - y_true, 0)\n",
        "    above = np.maximum(y_true - upper, 0)\n",
        "    return width + (2 / alpha) * (below + above)\n",
        "\n",
        "def oof_and_hill_climb_two_weights(X, y, model_lower, model_upper, alpha=0.1, n_splits=5, seed=42, steps=100):\n",
        "\n",
        "    y_raw = np.expm1(y)\n",
        "\n",
        "    oof_lowers = np.zeros(len(X))\n",
        "    oof_uppers = np.zeros(len(X))\n",
        "\n",
        "    fold_lower_models = []\n",
        "    fold_upper_models = []\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        lower_model = clone(model_lower)\n",
        "        upper_model = clone(model_upper)\n",
        "\n",
        "        callbacks = [\n",
        "        lgb.early_stopping(stopping_rounds=200),\n",
        "        lgb.log_evaluation(period=0)\n",
        "        ]\n",
        "\n",
        "\n",
        "        lower_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=\"quantile\",\n",
        "        callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        upper_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=\"quantile\",\n",
        "        callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        #oof_lowers[val_idx] = lower_model.predict(X_val)\n",
        "        #oof_uppers[val_idx] = upper_model.predict(X_val)\n",
        "\n",
        "        # － lower\n",
        "        oof_lowers[val_idx] = np.expm1(   # 👈 還原\n",
        "        lower_model.predict(X_val, num_iteration=lower_model.best_iteration_)\n",
        "        )\n",
        "        # － upper\n",
        "        oof_uppers[val_idx] = np.expm1(   # 👈 還原\n",
        "        upper_model.predict(X_val, num_iteration=upper_model.best_iteration_)\n",
        "        )\n",
        "\n",
        "        fold_lower_models.append(lower_model)\n",
        "        fold_upper_models.append(upper_model)\n",
        "\n",
        "\n",
        "    # 初始化雙權重\n",
        "    current_w1 = 0.4  # 下限 weight\n",
        "    current_w2 = 0.6  # 上限 weight\n",
        "\n",
        "    best_score = np.inf\n",
        "    best_weights = (current_w1, current_w2)\n",
        "\n",
        "    '''for step in range(steps):\n",
        "        # 微調 perturbation，讓 weight 有隨機性（避免卡住）\n",
        "        perturb1 = np.random.dirichlet([9])[0] - 0.9\n",
        "        perturb2 = np.random.dirichlet([9])[0] - 0.9\n",
        "\n",
        "        w1 = np.clip(current_w1 + 0.1 * perturb1, 0, 1)\n",
        "        w2 = np.clip(current_w2 + 0.1 * perturb2, 0, 1)\n",
        "\n",
        "        # 雙權重組合\n",
        "        lower_combined = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "        upper_combined = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "\n",
        "        # 修正：確保上下限方向正確（防止預測範圍錯位）\n",
        "        lower_combined, upper_combined = np.minimum(lower_combined, upper_combined), np.maximum(lower_combined, upper_combined)\n",
        "\n",
        "        score = np.mean(winkler_score(y_raw, lower_combined, upper_combined, alpha))\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_weights = (w1, w2)\n",
        "            current_w1, current_w2 = w1, w2\n",
        "            print(f\"[Step {step}] ✅ Improved Score: {best_score:.2f} (w1: {w1:.4f}, w2: {w2:.4f})\")'''\n",
        "    \n",
        "\n",
        "\n",
        "    grid = np.linspace(0, 1, 401)          # 0.0025 步\n",
        "    best_score = np.inf\n",
        "    best_weights = (0, 0)\n",
        "    best_cov = 0\n",
        "\n",
        "    y_raw = np.expm1(y)                    # 還原單位一次就好\n",
        "\n",
        "    for w1 in grid:\n",
        "        for w2 in grid:\n",
        "            if w1 > w2:                       # 保持下限≤上限\n",
        "                continue\n",
        "            low  = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "            high = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "\n",
        "            score = winkler_score(y_raw, low, high, alpha).mean()\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_weights = (w1, w2)\n",
        "                best_cov = ((y_raw >= low) & (y_raw <= high)).mean()\n",
        "\n",
        "    print(f\"[Grid] best Winkler {best_score:.0f}  \"\n",
        "          f\"w1={best_weights[0]:.3f}  w2={best_weights[1]:.3f}  \"\n",
        "          f\"cov={best_cov:.3f}\")\n",
        "\n",
        "    return (oof_lowers, oof_uppers, \n",
        "            best_weights, best_score , \n",
        "            fold_lower_models, fold_upper_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"lower\": lgb.LGBMRegressor(\n",
        "        objective=\"quantile\",\n",
        "        alpha=0.05,\n",
        "        device=\"cpu\",\n",
        "        n_estimators=8000,\n",
        "        learning_rate=0.03,\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        subsample_freq=1,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"upper\": lgb.LGBMRegressor(\n",
        "        objective=\"quantile\",\n",
        "        alpha=0.95,\n",
        "        device=\"cpu\",\n",
        "        n_estimators=8000,\n",
        "        learning_rate=0.03,\n",
        "        num_leaves=63,\n",
        "        subsample=0.8,\n",
        "        subsample_freq=1,\n",
        "        random_state=42\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.295253 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8040\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2022\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 12.128117\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1714]\tvalid_0's quantile: 0.0152108\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.284303 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8040\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2022\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 14.176676\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1899]\tvalid_0's quantile: 0.0132534\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.293105 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8022\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2018\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 12.128117\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1449]\tvalid_0's quantile: 0.0151327\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.327607 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8022\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2018\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 14.173185\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2002]\tvalid_0's quantile: 0.0131802\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.285291 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8043\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2026\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 12.128117\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1520]\tvalid_0's quantile: 0.0154222\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.389352 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8043\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2026\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 14.173884\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1744]\tvalid_0's quantile: 0.0132563\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.379682 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8010\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2013\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 12.128117\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1592]\tvalid_0's quantile: 0.0153088\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.394477 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8010\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2013\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 14.180155\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1961]\tvalid_0's quantile: 0.0130868\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.355525 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8026\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2020\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 12.128117\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1630]\tvalid_0's quantile: 0.0151627\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.350043 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8026\n",
            "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 2020\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 14.173185\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1825]\tvalid_0's quantile: 0.0130115\n",
            "[Grid] best Winkler 336938  w1=1.000  w2=1.000  cov=0.850\n",
            "OOF Winkler  = 336938\n",
            "最佳權重      = w1 1.000 / w2 1.000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'grid = np.linspace(0, 1, 401)\\nbest = (np.inf, 0, 0, 0)                   # (winkler, w1, w2, coverage)\\n\\nfor w1 in grid:\\n    for w2 in grid:\\n        if w1 <= w2:\\n            lower = w1 * oof_lowers + (1 - w1) * oof_uppers\\n            upper = w2 * oof_uppers + (1 - w2) * oof_lowers\\n\\n            cov   = ((y_raw >= lower) & (y_raw <= upper)).mean()\\n            \\n            score = winkler_score(y_raw, lower, upper).mean()\\n            if score < best[0]:\\n                best = (score, w1, w2, cov)\\n\\nprint(f\"best Winkler {best[0]:.0f}  w1={best[1]:.3f}  w2={best[2]:.3f}  cov={best[3]:.3f}\")'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oof_lowers, oof_uppers, best_weights, best_score , fold_lower_models, fold_upper_models= oof_and_hill_climb_two_weights(\n",
        "    X, y,\n",
        "    model_lower=models[\"lower\"],\n",
        "    model_upper=models[\"upper\"],\n",
        "    alpha=0.1, \n",
        "    n_splits=5,\n",
        "    steps=100\n",
        ")\n",
        "\n",
        "w1_opt, w2_opt = best_weights\n",
        "print(f\"OOF Winkler  = {best_score:.0f}\")\n",
        "print(f\"最佳權重      = w1 {w1_opt:.3f} / w2 {w2_opt:.3f}\")\n",
        "\n",
        "'''grid = np.linspace(0, 1, 401)\n",
        "best = (np.inf, 0, 0, 0)                   # (winkler, w1, w2, coverage)\n",
        "\n",
        "for w1 in grid:\n",
        "    for w2 in grid:\n",
        "        if w1 <= w2:\n",
        "            lower = w1 * oof_lowers + (1 - w1) * oof_uppers\n",
        "            upper = w2 * oof_uppers + (1 - w2) * oof_lowers\n",
        "\n",
        "            cov   = ((y_raw >= lower) & (y_raw <= upper)).mean()\n",
        "            \n",
        "            score = winkler_score(y_raw, lower, upper).mean()\n",
        "            if score < best[0]:\n",
        "                best = (score, w1, w2, cov)\n",
        "\n",
        "print(f\"best Winkler {best[0]:.0f}  w1={best[1]:.3f}  w2={best[2]:.3f}  cov={best[3]:.3f}\")'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "before plus floor ratio~bath per bed score = 341552  (original type)\n",
        "after plus new feature score = 342013\n",
        "after adjust model = 387816\n",
        "\n",
        "try to check coverage = 340265.61 (I put new feature and do pca but not doing stacking model) ##model alpha set as 0.05 & 0.95\n",
        "try to check coverage = 337115.51 (I put new feature and do pca but not doing stacking model) ##model alpha set as 0.05 & 0.95\n",
        "instead pca&kmean by geohash = 336938"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "離線測試"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5‑fold 平均 + 權重  ->  Winkler 265830 ,  coverage 0.882\n"
          ]
        }
      ],
      "source": [
        "# 先把真實價格還原到『元』\n",
        "y_raw = np.expm1(y)\n",
        "\n",
        "# 5 個模型各自預測整個 X，再平均\n",
        "oof_low_mean  = np.zeros(len(X))\n",
        "oof_high_mean = np.zeros(len(X))\n",
        "\n",
        "for lo, up in zip(fold_lower_models, fold_upper_models):\n",
        "    oof_low_mean  += np.expm1(lo.predict(X, num_iteration=lo.best_iteration_))\n",
        "    oof_high_mean += np.expm1(up.predict(X, num_iteration=up.best_iteration_))\n",
        "oof_low_mean  /= len(fold_lower_models)\n",
        "oof_high_mean /= len(fold_upper_models)\n",
        "\n",
        "# 套最佳權重\n",
        "low_final  = w1_opt * oof_low_mean  + (1 - w1_opt) * oof_high_mean\n",
        "high_final = w2_opt * oof_high_mean + (1 - w2_opt) * oof_low_mean\n",
        "low_final, high_final = np.minimum(low_final, high_final), np.maximum(low_final, high_final)\n",
        "\n",
        "# 計 coverage 與 Winkler\n",
        "cov_final = ((y_raw >= low_final) & (y_raw <= high_final)).mean()\n",
        "wink_final = winkler_score(y_raw, low_final, high_final, alpha=0.1).mean()\n",
        "\n",
        "print(f\"5‑fold 平均 + 權重  ->  Winkler {wink_final:.0f} ,  coverage {cov_final:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "輸出test LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#填補缺漏欄位（對齊訓練集欄位）\n",
        "missing_cols = set(X.columns) - set(test_encoded.columns)\n",
        "for col in missing_cols:\n",
        "    test_encoded[col] = 0\n",
        "\n",
        "# 確保欄位順序一致\n",
        "test_encoded = test_encoded[X.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 先確保 test_encoded 欄位與 X 完全一致\n",
        "test_encoded = test_encoded[X.columns]\n",
        "\n",
        "# ---------- 5‑fold 平均預測 ----------\n",
        "pred_low_raw  = np.zeros(len(test_encoded))\n",
        "pred_high_raw = np.zeros(len(test_encoded))\n",
        "\n",
        "for lo, up in zip(fold_lower_models, fold_upper_models):          # ← 名稱跟 return 一致\n",
        "    pred_low_raw  += np.expm1(lo.predict(test_encoded,  num_iteration=lo.best_iteration_))\n",
        "    pred_high_raw += np.expm1(up.predict(test_encoded, num_iteration=up.best_iteration_))\n",
        "\n",
        "pred_low_raw  /= len(fold_lower_models)\n",
        "pred_high_raw /= len(fold_upper_models)\n",
        "\n",
        "# ---------- 套用最佳權重 ----------\n",
        "test_lower = w1_opt * pred_low_raw  + (1 - w1_opt) * pred_high_raw\n",
        "test_upper = w2_opt * pred_high_raw + (1 - w2_opt) * pred_low_raw\n",
        "test_lower, test_upper = np.minimum(test_lower, test_upper), np.maximum(test_lower, test_upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df = pd.read_csv('sample_submission.csv')\n",
        "submission_df.head()\n",
        "test_encoded['id'] = test_df['id']  # 這行先補上 id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       id       pi_lower      pi_upper\n",
            "0  200000  822467.256382  1.124751e+06\n",
            "1  200001  553854.314402  7.305524e+05\n",
            "2  200002  427859.249775  6.403460e+05\n",
            "3  200003  326270.997287  4.379295e+05\n",
            "4  200004  380530.430672  6.281208e+05\n"
          ]
        }
      ],
      "source": [
        "submission_df = pd.DataFrame({\n",
        "    'id': test_encoded['id'],  # 必須與 sample_submission 對齊\n",
        "    'pi_lower': test_lower,\n",
        "    'pi_upper': test_upper\n",
        "})\n",
        "\n",
        "# 輸出成 CSV\n",
        "submission_df.to_csv('geo6_5fold.csv', index=False)\n",
        "print(submission_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_lower head : [822467.25638198 553854.31440239 427859.24977455]\n",
            "test_upper head : [1124750.84586707  730552.40696786  640346.00007794]\n",
            "lower <= upper? : True\n",
            "寬度平均        : 219392.98410764078\n"
          ]
        }
      ],
      "source": [
        "print(\"test_lower head :\", test_lower[:3])\n",
        "print(\"test_upper head :\", test_upper[:3])\n",
        "print(\"lower <= upper? :\", np.all(test_lower <= test_upper))\n",
        "print(\"寬度平均        :\", (test_upper - test_lower).mean())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_tensor_evn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
